{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ASSIGNMENT NO : 02\n",
        "\n",
        "*   Nooran Ishtiaq\n",
        "*   22i-2010\n",
        "*   DS-B\n",
        "\n"
      ],
      "metadata": {
        "id": "uPAX6Lze3GnC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kOmGi0DBAYn3"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "zip_file_path = '/content/archive.zip'\n",
        "extraction_path = '/content/csv/' # Changed extraction path to /content/csv/\n",
        "\n",
        "# Create the extraction directory if it doesn't exist\n",
        "os.makedirs(extraction_path, exist_ok=True)\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extraction_path)\n",
        "print(f'Archive extracted to: {extraction_path}')\n",
        "# List the contents of the extracted directory to confirm\n",
        "print('\\nContents of extracted directory:')\n",
        "for item in os.listdir(extraction_path):\n",
        "    print(item)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Baseline 1 :  BiLSTMEncoder"
      ],
      "metadata": {
        "id": "bdtWWXlM3bHx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import math\n",
        "import random\n",
        "import re\n",
        "from dataclasses import dataclass\n",
        "from pathlib import Path\n",
        "from typing import List, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        "    f1_score,\n",
        "    roc_auc_score,\n",
        "    average_precision_score,\n",
        ")\n",
        "\n",
        "# ---- Standalone utilities (inlined from baseline1) ----\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class Clause:\n",
        "    text: str\n",
        "    clause_type: str\n",
        "    source_file: str\n",
        "\n",
        "\n",
        "def set_global_seeds(seed: int) -> None:\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "\n",
        "def load_clauses_from_folder(folder: Path) -> List[Clause]:\n",
        "    csv_files = sorted(folder.rglob(\"*.csv\"))\n",
        "    clauses: List[Clause] = []\n",
        "    for csv_path in csv_files:\n",
        "        try:\n",
        "            df = pd.read_csv(csv_path)\n",
        "        except Exception as exc:\n",
        "            print(f\"Warning: failed to read {csv_path.name}: {exc}\")\n",
        "            continue\n",
        "\n",
        "        if \"clause_text\" not in df.columns or \"clause_type\" not in df.columns:\n",
        "            print(f\"Warning: {csv_path.name} does not contain required columns 'clause_text' and 'clause_type'. Skipping.\")\n",
        "            continue\n",
        "\n",
        "        for _, row in df.iterrows():\n",
        "            text = str(row[\"clause_text\"]) if not pd.isna(row[\"clause_text\"]) else \"\"\n",
        "            ctype = str(row[\"clause_type\"]) if not pd.isna(row[\"clause_type\"]) else \"\"\n",
        "            text = text.strip()\n",
        "            ctype = ctype.strip()\n",
        "            if text:\n",
        "                clauses.append(Clause(text=text, clause_type=ctype, source_file=csv_path.name))\n",
        "\n",
        "    return clauses\n",
        "\n",
        "\n",
        "def make_balanced_pairs(\n",
        "    clauses: List[Clause],\n",
        "    max_pairs_per_class: int | None,\n",
        "    rng: np.random.Generator,\n",
        ") -> Tuple[List[Tuple[str, str]], np.ndarray]:\n",
        "    type_to_indices: dict[str, List[int]] = {}\n",
        "    for idx, c in enumerate(clauses):\n",
        "        type_to_indices.setdefault(c.clause_type, []).append(idx)\n",
        "\n",
        "    unique_types = [t for t in type_to_indices.keys() if t]\n",
        "    if len(unique_types) < 2:\n",
        "        raise ValueError(\n",
        "            \"Found fewer than 2 distinct clause_type values across CSVs. \"\n",
        "            \"To build a similarity dataset, place multiple CSVs (or files having multiple clause_type values) in the folder.\"\n",
        "        )\n",
        "\n",
        "    positive_pairs: List[Tuple[int, int]] = []\n",
        "    for ctype, indices in type_to_indices.items():\n",
        "        if len(indices) < 2:\n",
        "            continue\n",
        "        shuffled = indices.copy()\n",
        "        rng.shuffle(shuffled)\n",
        "        pair_count = len(shuffled) // 2\n",
        "        if max_pairs_per_class is not None:\n",
        "            pair_count = min(pair_count, max_pairs_per_class)\n",
        "        for i in range(pair_count):\n",
        "            a = shuffled[2 * i]\n",
        "            b = shuffled[2 * i + 1]\n",
        "            if a != b:\n",
        "                positive_pairs.append((a, b))\n",
        "\n",
        "    if not positive_pairs:\n",
        "        raise ValueError(\"Could not form any positive pairs. Ensure each clause_type has at least two rows.\")\n",
        "\n",
        "    num_pos = len(positive_pairs)\n",
        "\n",
        "    all_indices = np.arange(len(clauses))\n",
        "    negative_pairs: List[Tuple[int, int]] = []\n",
        "    attempts = 0\n",
        "    while len(negative_pairs) < num_pos and attempts < num_pos * 20:\n",
        "        attempts += 1\n",
        "        i, j = rng.choice(all_indices, size=2, replace=False)\n",
        "        if clauses[i].clause_type != clauses[j].clause_type:\n",
        "            negative_pairs.append((i, j))\n",
        "\n",
        "    pairs_text: List[Tuple[str, str]] = []\n",
        "    labels: List[int] = []\n",
        "\n",
        "    for i, j in positive_pairs:\n",
        "        pairs_text.append((clauses[i].text, clauses[j].text))\n",
        "        labels.append(1)\n",
        "    for i, j in negative_pairs:\n",
        "        pairs_text.append((clauses[i].text, clauses[j].text))\n",
        "        labels.append(0)\n",
        "\n",
        "    order = np.arange(len(pairs_text))\n",
        "    rng.shuffle(order)\n",
        "    pairs_text = [pairs_text[k] for k in order]\n",
        "    y = np.asarray([labels[k] for k in order], dtype=np.int64)\n",
        "    return pairs_text, y\n",
        "\n",
        "\n",
        "def evaluate_binary(y_true: np.ndarray, y_proba: np.ndarray) -> dict:\n",
        "    y_pred = (y_proba >= 0.5).astype(int)\n",
        "    metrics = {\n",
        "        \"accuracy\": float(accuracy_score(y_true, y_pred)),\n",
        "        \"precision\": float(precision_score(y_true, y_pred, zero_division=0)),\n",
        "        \"recall\": float(recall_score(y_true, y_pred, zero_division=0)),\n",
        "        \"f1\": float(f1_score(y_true, y_pred, zero_division=0)),\n",
        "    }\n",
        "    try:\n",
        "        metrics[\"roc_auc\"] = float(roc_auc_score(y_true, y_proba))\n",
        "    except Exception:\n",
        "        metrics[\"roc_auc\"] = float(\"nan\")\n",
        "    try:\n",
        "        metrics[\"pr_auc\"] = float(average_precision_score(y_true, y_proba))\n",
        "    except Exception:\n",
        "        metrics[\"pr_auc\"] = float(\"nan\")\n",
        "    return metrics\n",
        "\n",
        "\n",
        "def find_best_threshold(y_true: np.ndarray, y_proba: np.ndarray, metric: str = \"f1\") -> float:\n",
        "    thresholds = np.linspace(0.05, 0.95, 19)\n",
        "    best_t = 0.5\n",
        "    best_score = -1.0\n",
        "    for t in thresholds:\n",
        "        y_pred = (y_proba >= t).astype(int)\n",
        "        if metric == \"accuracy\":\n",
        "            score = accuracy_score(y_true, y_pred)\n",
        "        else:\n",
        "            score = f1_score(y_true, y_pred, zero_division=0)\n",
        "        if score > best_score:\n",
        "            best_score = score\n",
        "            best_t = t\n",
        "    return float(best_t)\n",
        "\n",
        "\n",
        "def build_vocab(texts: List[str], max_vocab_size: int, min_freq: int) -> tuple[dict, dict]:\n",
        "    \"\"\"\n",
        "    Build a simple word-level vocabulary from training texts.\n",
        "    Returns (token_to_id, id_to_token). 0 is PAD, 1 is UNK.\n",
        "    \"\"\"\n",
        "    token_freq: dict[str, int] = {}\n",
        "    for t in texts:\n",
        "        for tok in re.findall(r\"\\b\\w+\\b\", t.lower()):\n",
        "            token_freq[tok] = token_freq.get(tok, 0) + 1\n",
        "    # Sort by frequency then lexicographically for determinism\n",
        "    sorted_tokens = sorted(\n",
        "        [tok for tok, f in token_freq.items() if f >= min_freq],\n",
        "        key=lambda x: (-token_freq[x], x),\n",
        "    )\n",
        "    # Reserve 0:PAD, 1:UNK\n",
        "    limited = sorted_tokens[: max(0, max_vocab_size - 2)]\n",
        "    token_to_id = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
        "    for i, tok in enumerate(limited, start=2):\n",
        "        token_to_id[tok] = i\n",
        "    id_to_token = {i: t for t, i in token_to_id.items()}\n",
        "    return token_to_id, id_to_token\n",
        "\n",
        "\n",
        "def texts_to_ids(texts: List[str], token_to_id: dict, max_len: int) -> tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Convert a list of texts to padded ID sequences and lengths.\n",
        "    Returns:\n",
        "        seqs: (N, max_len) int32\n",
        "        lens: (N,) int32\n",
        "    \"\"\"\n",
        "    unk_id = token_to_id.get(\"<UNK>\", 1)\n",
        "    pad_id = token_to_id.get(\"<PAD>\", 0)\n",
        "    seqs = np.full((len(texts), max_len), pad_id, dtype=np.int64)\n",
        "    lens = np.zeros((len(texts),), dtype=np.int64)\n",
        "    for i, t in enumerate(texts):\n",
        "        tokens = re.findall(r\"\\b\\w+\\b\", t.lower())\n",
        "        ids = [token_to_id.get(tok, unk_id) for tok in tokens][:max_len]\n",
        "        seqs[i, : len(ids)] = np.asarray(ids, dtype=np.int64)\n",
        "        lens[i] = len(ids)\n",
        "    return seqs, lens\n",
        "\n",
        "\n",
        "class PairDataset(Dataset):\n",
        "    def __init__(\n",
        "        self,\n",
        "        pairs: List[Tuple[str, str]],\n",
        "        token_to_id: dict,\n",
        "        max_len: int,\n",
        "        labels: np.ndarray | None = None,\n",
        "    ) -> None:\n",
        "        self.left_texts = [a for a, _ in pairs]\n",
        "        self.right_texts = [b for _, b in pairs]\n",
        "        self.labels = None if labels is None else labels.astype(np.int64)\n",
        "        self.token_to_id = token_to_id\n",
        "        self.max_len = max_len\n",
        "\n",
        "        self.left_ids, self.left_lens = texts_to_ids(self.left_texts, token_to_id, max_len)\n",
        "        self.right_ids, self.right_lens = texts_to_ids(self.right_texts, token_to_id, max_len)\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.left_texts)\n",
        "\n",
        "    def __getitem__(self, idx: int):\n",
        "        l_ids = torch.from_numpy(self.left_ids[idx])\n",
        "        r_ids = torch.from_numpy(self.right_ids[idx])\n",
        "        l_len = int(self.left_lens[idx])\n",
        "        r_len = int(self.right_lens[idx])\n",
        "        if self.labels is None:\n",
        "            return l_ids, l_len, r_ids, r_len\n",
        "        return l_ids, l_len, r_ids, r_len, int(self.labels[idx])\n",
        "\n",
        "\n",
        "class BiLSTMEncoder(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_size: int,\n",
        "        embedding_dim: int,\n",
        "        hidden_size: int,\n",
        "        num_layers: int,\n",
        "        pad_idx: int = 0,\n",
        "        dropout: float = 0.1,\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=embedding_dim,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            bidirectional=True,\n",
        "            dropout=dropout if num_layers > 1 else 0.0,\n",
        "        )\n",
        "\n",
        "    def forward(self, ids: torch.Tensor, lengths: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            ids: (B, T) long\n",
        "            lengths: (B,) long\n",
        "        Returns:\n",
        "            enc: (B, 2*hidden_size) final BiLSTM pooled representation\n",
        "        \"\"\"\n",
        "        embeds = self.embedding(ids)  # (B, T, D)\n",
        "        packed = pack_padded_sequence(embeds, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
        "        _, (h_n, _) = self.lstm(packed)\n",
        "        # h_n: (num_layers*2, B, H). Take last layer's forward and backward\n",
        "        # last forward = h_n[-2], last backward = h_n[-1]\n",
        "        h_forward = h_n[-2]\n",
        "        h_backward = h_n[-1]\n",
        "        enc = torch.cat([h_forward, h_backward], dim=1)  # (B, 2H)\n",
        "        return enc\n",
        "\n",
        "\n",
        "class SiameseBiLSTM(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_size: int,\n",
        "        embedding_dim: int,\n",
        "        hidden_size: int,\n",
        "        num_layers: int,\n",
        "        mlp_hidden: int,\n",
        "        pad_idx: int = 0,\n",
        "        dropout: float = 0.2,\n",
        "        use_cosine_feature: bool = True,\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "        self.encoder = BiLSTMEncoder(\n",
        "            vocab_size=vocab_size,\n",
        "            embedding_dim=embedding_dim,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=num_layers,\n",
        "            pad_idx=pad_idx,\n",
        "            dropout=dropout,\n",
        "        )\n",
        "        self.use_cosine_feature = use_cosine_feature\n",
        "\n",
        "        enc_dim = hidden_size * 2\n",
        "        feat_dim = enc_dim * 4  # [u, v, |u-v|, u*v]\n",
        "        if self.use_cosine_feature:\n",
        "            feat_dim += 1\n",
        "\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(feat_dim, mlp_hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(mlp_hidden, mlp_hidden // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(mlp_hidden // 2, 1),\n",
        "        )\n",
        "\n",
        "    @staticmethod\n",
        "    def _cosine(u: torch.Tensor, v: torch.Tensor, eps: float = 1e-12) -> torch.Tensor:\n",
        "        num = (u * v).sum(dim=1, keepdim=True)\n",
        "        den = (u.norm(dim=1, keepdim=True) * v.norm(dim=1, keepdim=True)).clamp_min(eps)\n",
        "        return num / den\n",
        "\n",
        "    def forward(self, l_ids: torch.Tensor, l_len: torch.Tensor, r_ids: torch.Tensor, r_len: torch.Tensor) -> torch.Tensor:\n",
        "        u = self.encoder(l_ids, l_len)  # (B, 2H)\n",
        "        v = self.encoder(r_ids, r_len)  # (B, 2H)\n",
        "        abs_diff = torch.abs(u - v)\n",
        "        hadamard = u * v\n",
        "        feats = [u, v, abs_diff, hadamard]\n",
        "        if self.use_cosine_feature:\n",
        "            cos = self._cosine(u, v)  # (B,1)\n",
        "            feats.append(cos)\n",
        "        x = torch.cat(feats, dim=1)\n",
        "        logits = self.mlp(x).squeeze(1)\n",
        "        return logits\n",
        "\n",
        "\n",
        "def evaluate_model(model: nn.Module, loader: DataLoader, device: torch.device) -> tuple[np.ndarray, np.ndarray]:\n",
        "    model.eval()\n",
        "    all_probs: list[float] = []\n",
        "    all_labels: list[int] = []\n",
        "    with torch.no_grad():\n",
        "        for batch in loader:\n",
        "            l_ids, l_len, r_ids, r_len, y = batch\n",
        "            l_ids = l_ids.to(device)\n",
        "            r_ids = r_ids.to(device)\n",
        "            l_len = l_len.to(device)\n",
        "            r_len = r_len.to(device)\n",
        "            y = y.to(device)\n",
        "            logits = model(l_ids, l_len, r_ids, r_len)\n",
        "            probs = torch.sigmoid(logits)\n",
        "            all_probs.extend(probs.cpu().numpy().tolist())\n",
        "            all_labels.extend(y.cpu().numpy().tolist())\n",
        "    return np.asarray(all_labels, dtype=np.int64), np.asarray(all_probs, dtype=np.float64)\n",
        "\n",
        "\n",
        "def run(\n",
        "    data_dir: Path,\n",
        "    seed: int,\n",
        "    max_pairs_per_class: int | None,\n",
        "    max_len: int,\n",
        "    max_vocab_size: int,\n",
        "    min_freq: int,\n",
        "    embedding_dim: int,\n",
        "    hidden_size: int,\n",
        "    num_layers: int,\n",
        "    mlp_hidden: int,\n",
        "    batch_size: int,\n",
        "    epochs: int,\n",
        "    lr: float,\n",
        "    log_interval: int,\n",
        ") -> None:\n",
        "    set_global_seeds(seed)\n",
        "    print(\"Loading clauses from:\", data_dir, flush=True)\n",
        "    clauses = load_clauses_from_folder(data_dir)\n",
        "    print(f\"Loaded {len(clauses)} clauses from {len(set(c.source_file for c in clauses))} CSV file(s).\", flush=True)\n",
        "\n",
        "    # Clause-level split first to avoid leakage of the same clause across splits\n",
        "    from sklearn.model_selection import train_test_split\n",
        "\n",
        "    clause_indices = np.arange(len(clauses))\n",
        "    clause_types = np.array([c.clause_type for c in clauses], dtype=object)\n",
        "\n",
        "    # Try stratified split by clause_type; if it fails due to rare classes, fall back to random split\n",
        "    try:\n",
        "        idx_train, idx_temp = train_test_split(\n",
        "            clause_indices, test_size=0.30, random_state=seed, stratify=clause_types\n",
        "        )\n",
        "        idx_val, idx_test = train_test_split(\n",
        "            idx_temp, test_size=0.50, random_state=seed, stratify=clause_types[idx_temp]\n",
        "        )\n",
        "    except Exception:\n",
        "        idx_train, idx_temp = train_test_split(clause_indices, test_size=0.30, random_state=seed)\n",
        "        idx_val, idx_test = train_test_split(idx_temp, test_size=0.50, random_state=seed)\n",
        "\n",
        "    clauses_train = [clauses[i] for i in idx_train]\n",
        "    clauses_val = [clauses[i] for i in idx_val]\n",
        "    clauses_test = [clauses[i] for i in idx_test]\n",
        "\n",
        "    # Build pairs independently within each split\n",
        "    rng = np.random.default_rng(seed)\n",
        "    X_train, y_train = make_balanced_pairs(clauses_train, max_pairs_per_class=max_pairs_per_class, rng=rng)\n",
        "    X_val, y_val = make_balanced_pairs(clauses_val, max_pairs_per_class=max_pairs_per_class, rng=rng)\n",
        "    X_test, y_test = make_balanced_pairs(clauses_test, max_pairs_per_class=max_pairs_per_class, rng=rng)\n",
        "\n",
        "    print(\n",
        "        f\"Split clauses -> train/val/test: {len(clauses_train)}/{len(clauses_val)}/{len(clauses_test)}. \"\n",
        "        f\"Built pairs -> train/val/test: {len(X_train)}/{len(X_val)}/{len(X_test)}.\",\n",
        "        flush=True,\n",
        "    )\n",
        "\n",
        "    # Build vocab on training texts only\n",
        "    train_texts = [a for a, _ in X_train] + [b for _, b in X_train]\n",
        "    token_to_id, _ = build_vocab(train_texts, max_vocab_size=max_vocab_size, min_freq=min_freq)\n",
        "    vocab_size = len(token_to_id)\n",
        "    print(f\"Vocab size: {vocab_size} (min_freq={min_freq}, max_vocab_size={max_vocab_size})\", flush=True)\n",
        "\n",
        "    # Datasets / Loaders\n",
        "    ds_train = PairDataset(X_train, token_to_id, max_len=max_len, labels=y_train)\n",
        "    ds_val = PairDataset(X_val, token_to_id, max_len=max_len, labels=y_val)\n",
        "    ds_test = PairDataset(X_test, token_to_id, max_len=max_len, labels=y_test)\n",
        "\n",
        "    def collate(batch):\n",
        "        l_ids, l_len, r_ids, r_len, y = zip(*batch)\n",
        "        return (\n",
        "            torch.stack(l_ids, dim=0),\n",
        "            torch.tensor(l_len, dtype=torch.long),\n",
        "            torch.stack(r_ids, dim=0),\n",
        "            torch.tensor(r_len, dtype=torch.long),\n",
        "            torch.tensor(y, dtype=torch.long),\n",
        "        )\n",
        "\n",
        "    train_loader = DataLoader(ds_train, batch_size=batch_size, shuffle=True, num_workers=0, collate_fn=collate)\n",
        "    val_loader = DataLoader(ds_val, batch_size=batch_size, shuffle=False, num_workers=0, collate_fn=collate)\n",
        "    test_loader = DataLoader(ds_test, batch_size=batch_size, shuffle=False, num_workers=0, collate_fn=collate)\n",
        "\n",
        "    # Model\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\", flush=True)\n",
        "    model = SiameseBiLSTM(\n",
        "        vocab_size=vocab_size,\n",
        "        embedding_dim=embedding_dim,\n",
        "        hidden_size=hidden_size,\n",
        "        num_layers=num_layers,\n",
        "        mlp_hidden=mlp_hidden,\n",
        "        pad_idx=0,\n",
        "        dropout=0.2,\n",
        "        use_cosine_feature=True,\n",
        "    ).to(device)\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    loss_fn = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    best_val_f1 = -1.0\n",
        "    best_state = None\n",
        "    total_batches = len(train_loader)\n",
        "    print(f\"Starting training for {epochs} epoch(s), steps per epoch: {total_batches}\", flush=True)\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        n_batches = 0\n",
        "        epoch_start = time.time()\n",
        "        for batch_idx, batch in enumerate(train_loader, start=1):\n",
        "            l_ids, l_len, r_ids, r_len, y = batch\n",
        "            l_ids = l_ids.to(device)\n",
        "            r_ids = r_ids.to(device)\n",
        "            l_len = l_len.to(device)\n",
        "            r_len = r_len.to(device)\n",
        "            y = y.float().to(device)\n",
        "\n",
        "            logits = model(l_ids, l_len, r_ids, r_len)\n",
        "            loss = loss_fn(logits, y)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += float(loss.item())\n",
        "            n_batches = batch_idx\n",
        "\n",
        "            if (batch_idx % max(1, log_interval) == 0) or (batch_idx == total_batches):\n",
        "                elapsed = time.time() - epoch_start\n",
        "                avg_loss = running_loss / n_batches\n",
        "                print(\n",
        "                    f\"Epoch {epoch:02d}/{epochs} - step {batch_idx:05d}/{total_batches:05d} \"\n",
        "                    f\"loss: {float(loss.item()):.4f} (avg: {avg_loss:.4f}) - elapsed: {elapsed:.1f}s\",\n",
        "                    flush=True,\n",
        "                )\n",
        "\n",
        "        # Validation\n",
        "        print(\"Evaluating on validation...\", flush=True)\n",
        "        y_val_true, y_val_prob = evaluate_model(model, val_loader, device)\n",
        "        val_metrics = evaluate_binary(y_val_true, y_val_prob)\n",
        "        if val_metrics[\"f1\"] > best_val_f1:\n",
        "            best_val_f1 = val_metrics[\"f1\"]\n",
        "            best_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
        "\n",
        "        avg_loss = running_loss / max(1, n_batches)\n",
        "        print(\n",
        "            f\"Epoch {epoch:02d}/{epochs} - loss: {avg_loss:.4f} - val_f1: {val_metrics['f1']:.4f} - val_acc: {val_metrics['accuracy']:.4f}\",\n",
        "            flush=True,\n",
        "        )\n",
        "\n",
        "    # Load best model\n",
        "    if best_state is not None:\n",
        "        model.load_state_dict(best_state)\n",
        "\n",
        "    # Evaluate with default 0.5\n",
        "    print(\"Final evaluation (val/test, t=0.50) ...\", flush=True)\n",
        "    y_val_true, y_val_prob = evaluate_model(model, val_loader, device)\n",
        "    y_test_true, y_test_prob = evaluate_model(model, test_loader, device)\n",
        "\n",
        "    val_metrics = evaluate_binary(y_val_true, y_val_prob)\n",
        "    test_metrics = evaluate_binary(y_test_true, y_test_prob)\n",
        "\n",
        "    print(\"\\nValidation metrics (t=0.50):\", flush=True)\n",
        "    for k, v in val_metrics.items():\n",
        "        print(f\"  {k:>9}: {v:.4f}\", flush=True)\n",
        "\n",
        "    # Tune threshold on validation to maximize F1 (reuse helper)\n",
        "    best_t = find_best_threshold(y_val_true, y_val_prob, metric=\"f1\")\n",
        "    print(f\"\\nChosen decision threshold from validation (max F1): t = {best_t:.2f}\", flush=True)\n",
        "\n",
        "    def with_threshold(y_true: np.ndarray, y_prob: np.ndarray, t: float) -> dict:\n",
        "        y_pred = (y_prob >= t).astype(int)\n",
        "        from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "        return {\n",
        "            \"accuracy\": float(accuracy_score(y_true, y_pred)),\n",
        "            \"precision\": float(precision_score(y_true, y_pred, zero_division=0)),\n",
        "            \"recall\": float(recall_score(y_true, y_pred, zero_division=0)),\n",
        "            \"f1\": float(f1_score(y_true, y_pred, zero_division=0)),\n",
        "        }\n",
        "\n",
        "    val_tuned = with_threshold(y_val_true, y_val_prob, best_t)\n",
        "    test_tuned = with_threshold(y_test_true, y_test_prob, best_t)\n",
        "\n",
        "    print(\"\\nValidation (tuned threshold):\", flush=True)\n",
        "    for k, v in val_tuned.items():\n",
        "        print(f\"  {k:>9}: {v:.4f}\", flush=True)\n",
        "\n",
        "    print(\"\\nTest (tuned threshold):\", flush=True)\n",
        "    for k, v in test_tuned.items():\n",
        "        print(f\"  {k:>9}: {v:.4f}\", flush=True)\n",
        "\n",
        "\n",
        "def main() -> None:\n",
        "    parser = argparse.ArgumentParser(description=\"Baseline 2: Siamese BiLSTM + MLP for legal clause similarity\")\n",
        "    parser.add_argument(\n",
        "        \"--data_dir\",\n",
        "        type=Path,\n",
        "        default=Path(\"csv\"),\n",
        "        help=\"Directory containing CSV files with columns: clause_text, clause_type (default: ./csv)\",\n",
        "    )\n",
        "    parser.add_argument(\"--seed\", type=int, default=42, help=\"Random seed\")\n",
        "    parser.add_argument(\n",
        "        \"--max_pairs_per_class\",\n",
        "        type=int,\n",
        "        default=200,\n",
        "        help=\"Upper bound of positive pairs sampled per clause_type (for speed/balance).\",\n",
        "    )\n",
        "    parser.add_argument(\"--max_len\", type=int, default=128, help=\"Maximum tokenized length per clause\")\n",
        "    parser.add_argument(\"--max_vocab_size\", type=int, default=30000, help=\"Maximum vocabulary size\")\n",
        "    parser.add_argument(\"--min_freq\", type=int, default=2, help=\"Minimum frequency for a token to enter the vocab\")\n",
        "\n",
        "    parser.add_argument(\"--embedding_dim\", type=int, default=200, help=\"Embedding dimension\")\n",
        "    parser.add_argument(\"--hidden_size\", type=int, default=128, help=\"BiLSTM hidden size (per direction)\")\n",
        "    parser.add_argument(\"--num_layers\", type=int, default=1, help=\"Number of LSTM layers\")\n",
        "    parser.add_argument(\"--mlp_hidden\", type=int, default=256, help=\"Hidden size of the MLP head\")\n",
        "\n",
        "    parser.add_argument(\"--batch_size\", type=int, default=64, help=\"Batch size\")\n",
        "    parser.add_argument(\"--epochs\", type=int, default=8, help=\"Training epochs\")\n",
        "    parser.add_argument(\"--lr\", type=float, default=1e-3, help=\"Learning rate\")\n",
        "    parser.add_argument(\"--log_interval\", type=int, default=200, help=\"Steps between training progress prints\")\n",
        "    args = parser.parse_args([]) # Modified to pass an empty list\n",
        "\n",
        "    run(\n",
        "        data_dir=args.data_dir,\n",
        "        seed=args.seed,\n",
        "        max_pairs_per_class=args.max_pairs_per_class,\n",
        "        max_len=args.max_len,\n",
        "        max_vocab_size=args.max_vocab_size,\n",
        "        min_freq=args.min_freq,\n",
        "        embedding_dim=args.embedding_dim,\n",
        "        hidden_size=args.hidden_size,\n",
        "        num_layers=args.num_layers,\n",
        "        mlp_hidden=args.mlp_hidden,\n",
        "        batch_size=args.batch_size,\n",
        "        epochs=args.epochs,\n",
        "        lr=args.lr,\n",
        "        log_interval=args.log_interval,\n",
        "    )\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h9Oz-vLZBTe9",
        "outputId": "71cb6335-7b0e-409d-9d70-74ab0f4e9def"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading clauses from: csv\n",
            "Loaded 150881 clauses from 395 CSV file(s).\n",
            "Split clauses -> train/val/test: 105616/22632/22633. Built pairs -> train/val/test: 105306/22438/22438.\n",
            "Vocab size: 30000 (min_freq=2, max_vocab_size=30000)\n",
            "Using device: cuda\n",
            "Starting training for 8 epoch(s), steps per epoch: 1646\n",
            "Epoch 01/8 - step 00200/01646 loss: 0.0011 (avg: 0.0550) - elapsed: 7.6s\n",
            "Epoch 01/8 - step 00400/01646 loss: 0.0015 (avg: 0.0319) - elapsed: 13.3s\n",
            "Epoch 01/8 - step 00600/01646 loss: 0.0026 (avg: 0.0255) - elapsed: 19.6s\n",
            "Epoch 01/8 - step 00800/01646 loss: 0.0002 (avg: 0.0211) - elapsed: 25.3s\n",
            "Epoch 01/8 - step 01000/01646 loss: 0.0014 (avg: 0.0182) - elapsed: 31.6s\n",
            "Epoch 01/8 - step 01200/01646 loss: 0.0027 (avg: 0.0164) - elapsed: 37.3s\n",
            "Epoch 01/8 - step 01400/01646 loss: 0.0004 (avg: 0.0151) - elapsed: 43.7s\n",
            "Epoch 01/8 - step 01600/01646 loss: 0.0017 (avg: 0.0142) - elapsed: 49.4s\n",
            "Epoch 01/8 - step 01646/01646 loss: 0.0000 (avg: 0.0138) - elapsed: 50.7s\n",
            "Evaluating on validation...\n",
            "Epoch 01/8 - loss: 0.0138 - val_f1: 0.9993 - val_acc: 0.9993\n",
            "Epoch 02/8 - step 00200/01646 loss: 0.0210 (avg: 0.0024) - elapsed: 5.9s\n",
            "Epoch 02/8 - step 00400/01646 loss: 0.0002 (avg: 0.0025) - elapsed: 11.9s\n",
            "Epoch 02/8 - step 00600/01646 loss: 0.0001 (avg: 0.0030) - elapsed: 18.0s\n",
            "Epoch 02/8 - step 00800/01646 loss: 0.0000 (avg: 0.0031) - elapsed: 23.8s\n",
            "Epoch 02/8 - step 01000/01646 loss: 0.0000 (avg: 0.0026) - elapsed: 29.9s\n",
            "Epoch 02/8 - step 01200/01646 loss: 0.0000 (avg: 0.0026) - elapsed: 35.6s\n",
            "Epoch 02/8 - step 01400/01646 loss: 0.0000 (avg: 0.0027) - elapsed: 41.9s\n",
            "Epoch 02/8 - step 01600/01646 loss: 0.0000 (avg: 0.0027) - elapsed: 47.6s\n",
            "Epoch 02/8 - step 01646/01646 loss: 0.0000 (avg: 0.0026) - elapsed: 49.4s\n",
            "Evaluating on validation...\n",
            "Epoch 02/8 - loss: 0.0026 - val_f1: 0.9993 - val_acc: 0.9993\n",
            "Epoch 03/8 - step 00200/01646 loss: 0.0000 (avg: 0.0017) - elapsed: 5.7s\n",
            "Epoch 03/8 - step 00400/01646 loss: 0.0000 (avg: 0.0015) - elapsed: 12.1s\n",
            "Epoch 03/8 - step 00600/01646 loss: 0.0001 (avg: 0.0013) - elapsed: 17.8s\n",
            "Epoch 03/8 - step 00800/01646 loss: 0.0069 (avg: 0.0011) - elapsed: 24.1s\n",
            "Epoch 03/8 - step 01000/01646 loss: 0.0005 (avg: 0.0010) - elapsed: 29.8s\n",
            "Epoch 03/8 - step 01200/01646 loss: 0.0000 (avg: 0.0010) - elapsed: 36.1s\n",
            "Epoch 03/8 - step 01400/01646 loss: 0.0000 (avg: 0.0009) - elapsed: 41.8s\n",
            "Epoch 03/8 - step 01600/01646 loss: 0.0001 (avg: 0.0010) - elapsed: 48.2s\n",
            "Epoch 03/8 - step 01646/01646 loss: 0.0001 (avg: 0.0010) - elapsed: 49.5s\n",
            "Evaluating on validation...\n",
            "Epoch 03/8 - loss: 0.0010 - val_f1: 0.9993 - val_acc: 0.9993\n",
            "Epoch 04/8 - step 00200/01646 loss: 0.0000 (avg: 0.0005) - elapsed: 6.3s\n",
            "Epoch 04/8 - step 00400/01646 loss: 0.0000 (avg: 0.0003) - elapsed: 12.0s\n",
            "Epoch 04/8 - step 00600/01646 loss: 0.0000 (avg: 0.0002) - elapsed: 18.3s\n",
            "Epoch 04/8 - step 00800/01646 loss: 0.0000 (avg: 0.0002) - elapsed: 24.1s\n",
            "Epoch 04/8 - step 01000/01646 loss: 0.0000 (avg: 0.0003) - elapsed: 30.3s\n",
            "Epoch 04/8 - step 01200/01646 loss: 0.0000 (avg: 0.0003) - elapsed: 36.0s\n",
            "Epoch 04/8 - step 01400/01646 loss: 0.0000 (avg: 0.0004) - elapsed: 42.1s\n",
            "Epoch 04/8 - step 01600/01646 loss: 0.0000 (avg: 0.0004) - elapsed: 48.1s\n",
            "Epoch 04/8 - step 01646/01646 loss: 0.0000 (avg: 0.0004) - elapsed: 49.4s\n",
            "Evaluating on validation...\n",
            "Epoch 04/8 - loss: 0.0004 - val_f1: 0.9995 - val_acc: 0.9995\n",
            "Epoch 05/8 - step 00200/01646 loss: 0.0001 (avg: 0.0016) - elapsed: 6.4s\n",
            "Epoch 05/8 - step 00400/01646 loss: 0.0000 (avg: 0.0012) - elapsed: 12.1s\n",
            "Epoch 05/8 - step 00600/01646 loss: 0.0000 (avg: 0.0013) - elapsed: 18.5s\n",
            "Epoch 05/8 - step 00800/01646 loss: 0.0000 (avg: 0.0011) - elapsed: 24.2s\n",
            "Epoch 05/8 - step 01000/01646 loss: 0.0000 (avg: 0.0011) - elapsed: 30.5s\n",
            "Epoch 05/8 - step 01200/01646 loss: 0.0000 (avg: 0.0011) - elapsed: 36.2s\n",
            "Epoch 05/8 - step 01400/01646 loss: 0.0000 (avg: 0.0009) - elapsed: 42.5s\n",
            "Epoch 05/8 - step 01600/01646 loss: 0.0000 (avg: 0.0009) - elapsed: 48.3s\n",
            "Epoch 05/8 - step 01646/01646 loss: 0.0000 (avg: 0.0009) - elapsed: 49.6s\n",
            "Evaluating on validation...\n",
            "Epoch 05/8 - loss: 0.0009 - val_f1: 0.9988 - val_acc: 0.9988\n",
            "Epoch 06/8 - step 00200/01646 loss: 0.0000 (avg: 0.0010) - elapsed: 5.7s\n",
            "Epoch 06/8 - step 00400/01646 loss: 0.0001 (avg: 0.0006) - elapsed: 12.0s\n",
            "Epoch 06/8 - step 00600/01646 loss: 0.0001 (avg: 0.0010) - elapsed: 17.7s\n",
            "Epoch 06/8 - step 00800/01646 loss: 0.0000 (avg: 0.0008) - elapsed: 24.1s\n",
            "Epoch 06/8 - step 01000/01646 loss: 0.0000 (avg: 0.0009) - elapsed: 29.9s\n",
            "Epoch 06/8 - step 01200/01646 loss: 0.0000 (avg: 0.0008) - elapsed: 36.3s\n",
            "Epoch 06/8 - step 01400/01646 loss: 0.0000 (avg: 0.0010) - elapsed: 42.1s\n",
            "Epoch 06/8 - step 01600/01646 loss: 0.0016 (avg: 0.0010) - elapsed: 48.4s\n",
            "Epoch 06/8 - step 01646/01646 loss: 0.0005 (avg: 0.0010) - elapsed: 49.7s\n",
            "Evaluating on validation...\n",
            "Epoch 06/8 - loss: 0.0010 - val_f1: 0.9994 - val_acc: 0.9994\n",
            "Epoch 07/8 - step 00200/01646 loss: 0.0007 (avg: 0.0009) - elapsed: 6.1s\n",
            "Epoch 07/8 - step 00400/01646 loss: 0.0000 (avg: 0.0004) - elapsed: 12.0s\n",
            "Epoch 07/8 - step 00600/01646 loss: 0.0000 (avg: 0.0003) - elapsed: 18.0s\n",
            "Epoch 07/8 - step 00800/01646 loss: 0.0000 (avg: 0.0002) - elapsed: 24.1s\n",
            "Epoch 07/8 - step 01000/01646 loss: 0.0000 (avg: 0.0002) - elapsed: 30.0s\n",
            "Epoch 07/8 - step 01200/01646 loss: 0.0000 (avg: 0.0002) - elapsed: 36.1s\n",
            "Epoch 07/8 - step 01400/01646 loss: 0.0000 (avg: 0.0001) - elapsed: 42.0s\n",
            "Epoch 07/8 - step 01600/01646 loss: 0.0000 (avg: 0.0001) - elapsed: 48.2s\n",
            "Epoch 07/8 - step 01646/01646 loss: 0.0000 (avg: 0.0001) - elapsed: 49.5s\n",
            "Evaluating on validation...\n",
            "Epoch 07/8 - loss: 0.0001 - val_f1: 0.9994 - val_acc: 0.9994\n",
            "Epoch 08/8 - step 00200/01646 loss: 0.0000 (avg: 0.0000) - elapsed: 6.3s\n",
            "Epoch 08/8 - step 00400/01646 loss: 0.0000 (avg: 0.0000) - elapsed: 12.1s\n",
            "Epoch 08/8 - step 00600/01646 loss: 0.0000 (avg: 0.0000) - elapsed: 18.4s\n",
            "Epoch 08/8 - step 00800/01646 loss: 0.0000 (avg: 0.0000) - elapsed: 24.0s\n",
            "Epoch 08/8 - step 01000/01646 loss: 0.0000 (avg: 0.0000) - elapsed: 30.3s\n",
            "Epoch 08/8 - step 01200/01646 loss: 0.0000 (avg: 0.0000) - elapsed: 36.0s\n",
            "Epoch 08/8 - step 01400/01646 loss: 0.0000 (avg: 0.0000) - elapsed: 42.3s\n",
            "Epoch 08/8 - step 01600/01646 loss: 0.0000 (avg: 0.0000) - elapsed: 47.9s\n",
            "Epoch 08/8 - step 01646/01646 loss: 0.0000 (avg: 0.0000) - elapsed: 49.2s\n",
            "Evaluating on validation...\n",
            "Epoch 08/8 - loss: 0.0000 - val_f1: 0.9994 - val_acc: 0.9994\n",
            "Final evaluation (val/test, t=0.50) ...\n",
            "\n",
            "Validation metrics (t=0.50):\n",
            "   accuracy: 0.9995\n",
            "  precision: 0.9993\n",
            "     recall: 0.9997\n",
            "         f1: 0.9995\n",
            "    roc_auc: 0.9998\n",
            "     pr_auc: 0.9997\n",
            "\n",
            "Chosen decision threshold from validation (max F1): t = 0.05\n",
            "\n",
            "Validation (tuned threshold):\n",
            "   accuracy: 0.9996\n",
            "  precision: 0.9993\n",
            "     recall: 0.9998\n",
            "         f1: 0.9996\n",
            "\n",
            "Test (tuned threshold):\n",
            "   accuracy: 0.9995\n",
            "  precision: 0.9991\n",
            "     recall: 0.9998\n",
            "         f1: 0.9995\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Baseline 2 :  TextCNNEncoder"
      ],
      "metadata": {
        "id": "quI0Rgzr3m8X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import random\n",
        "import re\n",
        "import time\n",
        "from dataclasses import dataclass\n",
        "from pathlib import Path\n",
        "from typing import List, Tuple, Literal\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        "    f1_score,\n",
        "    roc_auc_score,\n",
        "    average_precision_score,\n",
        ")\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Standalone utilities (shared)\n",
        "# -----------------------------\n",
        "@dataclass\n",
        "class Clause:\n",
        "    text: str\n",
        "    clause_type: str\n",
        "    source_file: str\n",
        "\n",
        "\n",
        "def set_global_seeds(seed: int) -> None:\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "\n",
        "def load_clauses_from_folder(folder: Path) -> List[Clause]:\n",
        "    csv_files = sorted(folder.rglob(\"*.csv\"))\n",
        "    clauses: List[Clause] = []\n",
        "    for csv_path in csv_files:\n",
        "        try:\n",
        "            df = pd.read_csv(csv_path)\n",
        "        except Exception as exc:\n",
        "            print(f\"Warning: failed to read {csv_path.name}: {exc}\")\n",
        "            continue\n",
        "        if \"clause_text\" not in df.columns or \"clause_type\" not in df.columns:\n",
        "            print(f\"Warning: {csv_path.name} missing columns 'clause_text'/'clause_type'. Skipping.\")\n",
        "            continue\n",
        "        for _, row in df.iterrows():\n",
        "            text = \"\" if pd.isna(row[\"clause_text\"]) else str(row[\"clause_text\"]).strip()\n",
        "            ctype = \"\" if pd.isna(row[\"clause_type\"]) else str(row[\"clause_type\"]).strip()\n",
        "            if text:\n",
        "                clauses.append(Clause(text=text, clause_type=ctype, source_file=csv_path.name))\n",
        "    return clauses\n",
        "\n",
        "\n",
        "def normalize_text(t: str) -> str:\n",
        "    return re.sub(r\"\\s+\", \" \", t.lower()).strip()\n",
        "\n",
        "\n",
        "def deduplicate_clauses(clauses: List[Clause]) -> List[Clause]:\n",
        "    seen: set[str] = set()\n",
        "    unique: List[Clause] = []\n",
        "    for c in clauses:\n",
        "        key = normalize_text(c.text)\n",
        "        if key in seen:\n",
        "            continue\n",
        "        seen.add(key)\n",
        "        unique.append(c)\n",
        "    return unique\n",
        "\n",
        "\n",
        "def check_overlap(a: List[Clause], b: List[Clause], name_a: str, name_b: str) -> None:\n",
        "    a_set = {normalize_text(c.text) for c in a}\n",
        "    b_set = {normalize_text(c.text) for c in b}\n",
        "    inter = a_set & b_set\n",
        "    if inter:\n",
        "        sample = list(inter)[:5]\n",
        "        print(f\"Warning: Found {len(inter)} overlapping clause_texts between {name_a} and {name_b}. e.g.: {sample}\")\n",
        "    else:\n",
        "        print(f\"No overlapping clause_texts between {name_a} and {name_b}.\")\n",
        "\n",
        "\n",
        "def make_balanced_pairs(\n",
        "    clauses: List[Clause],\n",
        "    max_pairs_per_class: int | None,\n",
        "    rng: np.random.Generator,\n",
        ") -> Tuple[List[Tuple[str, str]], np.ndarray]:\n",
        "    type_to_indices: dict[str, List[int]] = {}\n",
        "    for idx, c in enumerate(clauses):\n",
        "        type_to_indices.setdefault(c.clause_type, []).append(idx)\n",
        "    unique_types = [t for t in type_to_indices.keys() if t]\n",
        "    if len(unique_types) < 2:\n",
        "        raise ValueError(\"Need at least two distinct clause_type values to form pairs.\")\n",
        "\n",
        "    positives: List[Tuple[int, int]] = []\n",
        "    for ctype, idxs in type_to_indices.items():\n",
        "        if len(idxs) < 2:\n",
        "            continue\n",
        "        pool = idxs.copy()\n",
        "        rng.shuffle(pool)\n",
        "        count = len(pool) // 2\n",
        "        if max_pairs_per_class is not None:\n",
        "            count = min(count, max_pairs_per_class)\n",
        "        for i in range(count):\n",
        "            a, b = pool[2 * i], pool[2 * i + 1]\n",
        "            if a != b:\n",
        "                positives.append((a, b))\n",
        "    if not positives:\n",
        "        raise ValueError(\"Could not form positive pairs inside split.\")\n",
        "\n",
        "    negatives: List[Tuple[int, int]] = []\n",
        "    all_idx = np.arange(len(clauses))\n",
        "    attempts = 0\n",
        "    while len(negatives) < len(positives) and attempts < len(positives) * 20:\n",
        "        attempts += 1\n",
        "        i, j = rng.choice(all_idx, size=2, replace=False)\n",
        "        if clauses[i].clause_type != clauses[j].clause_type:\n",
        "            negatives.append((i, j))\n",
        "\n",
        "    pairs: List[Tuple[str, str]] = []\n",
        "    labels: List[int] = []\n",
        "    for i, j in positives:\n",
        "        pairs.append((clauses[i].text, clauses[j].text))\n",
        "        labels.append(1)\n",
        "    for i, j in negatives:\n",
        "        pairs.append((clauses[i].text, clauses[j].text))\n",
        "        labels.append(0)\n",
        "    order = np.arange(len(pairs))\n",
        "    rng.shuffle(order)\n",
        "    pairs = [pairs[k] for k in order]\n",
        "    y = np.asarray([labels[k] for k in order], dtype=np.int64)\n",
        "    return pairs, y\n",
        "\n",
        "\n",
        "def evaluate_binary(y_true: np.ndarray, y_proba: np.ndarray) -> dict:\n",
        "    y_pred = (y_proba >= 0.5).astype(int)\n",
        "    out = {\n",
        "        \"accuracy\": float(accuracy_score(y_true, y_pred)),\n",
        "        \"precision\": float(precision_score(y_true, y_pred, zero_division=0)),\n",
        "        \"recall\": float(recall_score(y_true, y_pred, zero_division=0)),\n",
        "        \"f1\": float(f1_score(y_true, y_pred, zero_division=0)),\n",
        "    }\n",
        "    try:\n",
        "        out[\"roc_auc\"] = float(roc_auc_score(y_true, y_proba))\n",
        "    except Exception:\n",
        "        out[\"roc_auc\"] = float(\"nan\")\n",
        "    try:\n",
        "        out[\"pr_auc\"] = float(average_precision_score(y_true, y_proba))\n",
        "    except Exception:\n",
        "        out[\"pr_auc\"] = float(\"nan\")\n",
        "    return out\n",
        "\n",
        "\n",
        "def find_best_threshold(y_true: np.ndarray, y_proba: np.ndarray, metric: str = \"f1\") -> float:\n",
        "    thresholds = np.linspace(0.05, 0.95, 19)\n",
        "    best_t, best_score = 0.5, -1.0\n",
        "    for t in thresholds:\n",
        "        y_pred = (y_proba >= t).astype(int)\n",
        "        score = accuracy_score(y_true, y_pred) if metric == \"accuracy\" else f1_score(y_true, y_pred, zero_division=0)\n",
        "        if score > best_score:\n",
        "            best_score, best_t = score, t\n",
        "    return float(best_t)\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Tokenizer / Dataset\n",
        "# -----------------------------\n",
        "def build_vocab(texts: List[str], max_vocab_size: int, min_freq: int) -> tuple[dict, dict]:\n",
        "    freq: dict[str, int] = {}\n",
        "    for t in texts:\n",
        "        for tok in re.findall(r\"\\b\\w+\\b\", t.lower()):\n",
        "            freq[tok] = freq.get(tok, 0) + 1\n",
        "    sorted_tokens = sorted([t for t, f in freq.items() if f >= min_freq], key=lambda x: (-freq[x], x))\n",
        "    tok2id = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
        "    for i, tok in enumerate(sorted_tokens[: max(0, max_vocab_size - 2)], start=2):\n",
        "        tok2id[tok] = i\n",
        "    id2tok = {i: t for t, i in tok2id.items()}\n",
        "    return tok2id, id2tok\n",
        "\n",
        "\n",
        "def texts_to_ids(texts: List[str], tok2id: dict, max_len: int) -> tuple[np.ndarray, np.ndarray]:\n",
        "    pad_id, unk_id = tok2id.get(\"<PAD>\", 0), tok2id.get(\"<UNK>\", 1)\n",
        "    seqs = np.full((len(texts), max_len), pad_id, dtype=np.int64)\n",
        "    lens = np.zeros((len(texts),), dtype=np.int64)\n",
        "    for i, t in enumerate(texts):\n",
        "        ids = [tok2id.get(tok, unk_id) for tok in re.findall(r\"\\b\\w+\\b\", t.lower())][:max_len]\n",
        "        seqs[i, : len(ids)] = np.asarray(ids, dtype=np.int64)\n",
        "        lens[i] = len(ids)\n",
        "    return seqs, lens\n",
        "\n",
        "\n",
        "class PairDataset(Dataset):\n",
        "    def __init__(self, pairs: List[Tuple[str, str]], tok2id: dict, max_len: int, labels: np.ndarray | None = None) -> None:\n",
        "        self.left = [a for a, _ in pairs]\n",
        "        self.right = [b for _, b in pairs]\n",
        "        self.labels = None if labels is None else labels.astype(np.int64)\n",
        "        self.left_ids, self.left_lens = texts_to_ids(self.left, tok2id, max_len)\n",
        "        self.right_ids, self.right_lens = texts_to_ids(self.right, tok2id, max_len)\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.left)\n",
        "\n",
        "    def __getitem__(self, idx: int):\n",
        "        l_ids = torch.from_numpy(self.left_ids[idx])\n",
        "        r_ids = torch.from_numpy(self.right_ids[idx])\n",
        "        l_len = int(self.left_lens[idx])\n",
        "        r_len = int(self.right_lens[idx])\n",
        "        if self.labels is None:\n",
        "            return l_ids, l_len, r_ids, r_len\n",
        "        return l_ids, l_len, r_ids, r_len, int(self.labels[idx])\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Model: TextCNN Encoder\n",
        "# -----------------------------\n",
        "class TextCNNEncoder(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_size: int,\n",
        "        embedding_dim: int,\n",
        "        num_channels: int,\n",
        "        kernel_sizes: List[int],\n",
        "        pad_idx: int = 0,\n",
        "        dropout: float = 0.1,\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
        "        self.convs = nn.ModuleList(\n",
        "            [nn.Conv1d(in_channels=embedding_dim, out_channels=num_channels, kernel_size=k) for k in kernel_sizes]\n",
        "        )\n",
        "        self.activation = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.output_dim = num_channels * len(kernel_sizes)\n",
        "\n",
        "    def forward(self, ids: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            ids: (B, T) long\n",
        "        Returns:\n",
        "            enc: (B, output_dim) - concatenated max-pooled feature maps\n",
        "        \"\"\"\n",
        "        x = self.embedding(ids)  # (B, T, D)\n",
        "        x = x.transpose(1, 2)    # (B, D, T) for Conv1d\n",
        "        pooled_outputs = []\n",
        "        for conv in self.convs:\n",
        "            h = self.activation(conv(x))         # (B, C, T')\n",
        "            h = torch.max(h, dim=2).values       # Global max pool over time -> (B, C)\n",
        "            pooled_outputs.append(h)\n",
        "        out = torch.cat(pooled_outputs, dim=1)   # (B, C * num_kernels)\n",
        "        out = self.dropout(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class SiameseTextCNN(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_size: int,\n",
        "        embedding_dim: int,\n",
        "        num_channels: int,\n",
        "        kernel_sizes: List[int],\n",
        "        mlp_hidden: int,\n",
        "        pad_idx: int = 0,\n",
        "        dropout: float = 0.2,\n",
        "        use_cosine_feature: bool = True,\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "        self.encoder = TextCNNEncoder(\n",
        "            vocab_size=vocab_size,\n",
        "            embedding_dim=embedding_dim,\n",
        "            num_channels=num_channels,\n",
        "            kernel_sizes=kernel_sizes,\n",
        "            pad_idx=pad_idx,\n",
        "            dropout=dropout,\n",
        "        )\n",
        "        self.use_cosine_feature = use_cosine_feature\n",
        "        enc_dim = self.encoder.output_dim\n",
        "        feat_dim = enc_dim * 4 + (1 if use_cosine_feature else 0)  # [u, v, |u-v|, u*v, cos]\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(feat_dim, mlp_hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(mlp_hidden, mlp_hidden // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(mlp_hidden // 2, 1),\n",
        "        )\n",
        "\n",
        "    @staticmethod\n",
        "    def _cosine(u: torch.Tensor, v: torch.Tensor, eps: float = 1e-12) -> torch.Tensor:\n",
        "        return (u * v).sum(dim=1, keepdim=True) / (u.norm(dim=1, keepdim=True) * v.norm(dim=1, keepdim=True)).clamp_min(eps)\n",
        "\n",
        "    def forward(self, l_ids: torch.Tensor, r_ids: torch.Tensor) -> torch.Tensor:\n",
        "        u = self.encoder(l_ids)   # (B, enc_dim)\n",
        "        v = self.encoder(r_ids)   # (B, enc_dim)\n",
        "        feats = [u, v, torch.abs(u - v), u * v]\n",
        "        if self.use_cosine_feature:\n",
        "            feats.append(self._cosine(u, v))\n",
        "        x = torch.cat(feats, dim=1)\n",
        "        return self.mlp(x).squeeze(1)\n",
        "\n",
        "\n",
        "def evaluate_model(model: nn.Module, loader: DataLoader, device: torch.device) -> tuple[np.ndarray, np.ndarray]:\n",
        "    model.eval()\n",
        "    all_probs: list[float] = []\n",
        "    all_labels: list[int] = []\n",
        "    with torch.no_grad():\n",
        "        for l_ids, l_len, r_ids, r_len, y in loader:\n",
        "            l_ids = l_ids.to(device)\n",
        "            r_ids = r_ids.to(device)\n",
        "            y = y.to(device)\n",
        "            probs = torch.sigmoid(model(l_ids, r_ids))\n",
        "            all_probs.extend(probs.cpu().numpy().tolist())\n",
        "            all_labels.extend(y.cpu().numpy().tolist())\n",
        "    return np.asarray(all_labels, dtype=np.int64), np.asarray(all_probs, dtype=np.float64)\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Run pipeline\n",
        "# -----------------------------\n",
        "def run(\n",
        "    data_dir: Path,\n",
        "    seed: int,\n",
        "    split_mode: Literal[\"clause\", \"type\"],\n",
        "    dedup_texts: bool,\n",
        "    max_pairs_per_class: int | None,\n",
        "    max_len: int,\n",
        "    max_vocab_size: int,\n",
        "    min_freq: int,\n",
        "    embedding_dim: int,\n",
        "    cnn_channels: int,\n",
        "    kernel_sizes: List[int],\n",
        "    mlp_hidden: int,\n",
        "    batch_size: int,\n",
        "    epochs: int,\n",
        "    lr: float,\n",
        "    log_interval: int,\n",
        ") -> None:\n",
        "    set_global_seeds(seed)\n",
        "    print(\"Loading clauses from:\", data_dir, flush=True)\n",
        "    clauses = load_clauses_from_folder(data_dir)\n",
        "    print(f\"Loaded {len(clauses)} clauses from {len(set(c.source_file for c in clauses))} CSV file(s).\", flush=True)\n",
        "\n",
        "    if dedup_texts:\n",
        "        before = len(clauses)\n",
        "        clauses = deduplicate_clauses(clauses)\n",
        "        after = len(clauses)\n",
        "        print(f\"Deduplicated exact texts: {before} -> {after}\", flush=True)\n",
        "\n",
        "    from sklearn.model_selection import train_test_split\n",
        "\n",
        "    rng = np.random.default_rng(seed)\n",
        "    if split_mode == \"clause\":\n",
        "        clause_indices = np.arange(len(clauses))\n",
        "        clause_types = np.array([c.clause_type for c in clauses], dtype=object)\n",
        "        try:\n",
        "            idx_train, idx_temp = train_test_split(clause_indices, test_size=0.30, random_state=seed, stratify=clause_types)\n",
        "            idx_val, idx_test = train_test_split(idx_temp, test_size=0.50, random_state=seed, stratify=clause_types[idx_temp])\n",
        "        except Exception:\n",
        "            idx_train, idx_temp = train_test_split(clause_indices, test_size=0.30, random_state=seed)\n",
        "            idx_val, idx_test = train_test_split(idx_temp, test_size=0.50, random_state=seed)\n",
        "        clauses_train = [clauses[i] for i in idx_train]\n",
        "        clauses_val = [clauses[i] for i in idx_val]\n",
        "        clauses_test = [clauses[i] for i in idx_test]\n",
        "    else:\n",
        "        all_types = sorted({c.clause_type for c in clauses if c.clause_type})\n",
        "        rng.shuffle(all_types)\n",
        "        n_types = len(all_types)\n",
        "        n_train = int(0.7 * n_types)\n",
        "        n_val = int(0.15 * n_types)\n",
        "        train_types = set(all_types[:n_train])\n",
        "        val_types = set(all_types[n_train:n_train + n_val])\n",
        "        test_types = set(all_types[n_train + n_val:])\n",
        "        clauses_train = [c for c in clauses if c.clause_type in train_types]\n",
        "        clauses_val = [c for c in clauses if c.clause_type in val_types]\n",
        "        clauses_test = [c for c in clauses if c.clause_type in test_types]\n",
        "\n",
        "    check_overlap(clauses_train, clauses_val, \"train\", \"val\")\n",
        "    check_overlap(clauses_train, clauses_test, \"train\", \"test\")\n",
        "    check_overlap(clauses_val, clauses_test, \"val\", \"test\")\n",
        "\n",
        "    X_train, y_train = make_balanced_pairs(clauses_train, max_pairs_per_class=max_pairs_per_class, rng=rng)\n",
        "    X_val, y_val = make_balanced_pairs(clauses_val, max_pairs_per_class=max_pairs_per_class, rng=rng)\n",
        "    X_test, y_test = make_balanced_pairs(clauses_test, max_pairs_per_class=max_pairs_per_class, rng=rng)\n",
        "    print(\n",
        "        f\"Split clauses -> train/val/test: {len(clauses_train)}/{len(clauses_val)}/{len(clauses_test)}. \"\n",
        "        f\"Built pairs -> train/val/test: {len(X_train)}/{len(X_val)}/{len(X_test)}.\",\n",
        "        flush=True,\n",
        "    )\n",
        "\n",
        "    train_texts = [a for a, _ in X_train] + [b for _, b in X_train]\n",
        "    tok2id, _ = build_vocab(train_texts, max_vocab_size=max_vocab_size, min_freq=min_freq)\n",
        "    vocab_size = len(tok2id)\n",
        "    print(f\"Vocab size: {vocab_size} (min_freq={min_freq}, max_vocab_size={max_vocab_size})\", flush=True)\n",
        "\n",
        "    ds_train = PairDataset(X_train, tok2id, max_len=max_len, labels=y_train)\n",
        "    ds_val = PairDataset(X_val, tok2id, max_len=max_len, labels=y_val)\n",
        "    ds_test = PairDataset(X_test, tok2id, max_len=max_len, labels=y_test)\n",
        "\n",
        "    def collate(batch):\n",
        "        l_ids, l_len, r_ids, r_len, y = zip(*batch)\n",
        "        return (\n",
        "            torch.stack(l_ids, dim=0),\n",
        "            torch.tensor(l_len, dtype=torch.long),\n",
        "            torch.stack(r_ids, dim=0),\n",
        "            torch.tensor(r_len, dtype=torch.long),\n",
        "            torch.tensor(y, dtype=torch.long),\n",
        "        )\n",
        "\n",
        "    train_loader = DataLoader(ds_train, batch_size=batch_size, shuffle=True, num_workers=0, collate_fn=collate)\n",
        "    val_loader = DataLoader(ds_val, batch_size=batch_size, shuffle=False, num_workers=0, collate_fn=collate)\n",
        "    test_loader = DataLoader(ds_test, batch_size=batch_size, shuffle=False, num_workers=0, collate_fn=collate)\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\", flush=True)\n",
        "    model = SiameseTextCNN(\n",
        "        vocab_size=vocab_size,\n",
        "        embedding_dim=embedding_dim,\n",
        "        num_channels=cnn_channels,\n",
        "        kernel_sizes=kernel_sizes,\n",
        "        mlp_hidden=mlp_hidden,\n",
        "        pad_idx=0,\n",
        "        dropout=0.2,\n",
        "        use_cosine_feature=True,\n",
        "    ).to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    loss_fn = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    best_val_f1 = -1.0\n",
        "    best_state = None\n",
        "    steps_per_epoch = len(train_loader)\n",
        "    print(f\"Starting training for {epochs} epoch(s), steps per epoch: {steps_per_epoch}\", flush=True)\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        epoch_start = time.time()\n",
        "        for step, batch in enumerate(train_loader, start=1):\n",
        "            l_ids, l_len, r_ids, r_len, y = batch\n",
        "            l_ids = l_ids.to(device)\n",
        "            r_ids = r_ids.to(device)\n",
        "            y = y.float().to(device)\n",
        "\n",
        "            logits = model(l_ids, r_ids)\n",
        "            loss = loss_fn(logits, y)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += float(loss.item())\n",
        "            if (step % max(1, log_interval) == 0) or (step == steps_per_epoch):\n",
        "                avg_loss = running_loss / step\n",
        "                elapsed = time.time() - epoch_start\n",
        "                print(\n",
        "                    f\"Epoch {epoch:02d}/{epochs} - step {step:05d}/{steps_per_epoch:05d} \"\n",
        "                    f\"loss: {float(loss.item()):.4f} (avg: {avg_loss:.4f}) - elapsed: {elapsed:.1f}s\",\n",
        "                    flush=True,\n",
        "                )\n",
        "\n",
        "        print(\"Evaluating on validation...\", flush=True)\n",
        "        y_val_true, y_val_prob = evaluate_model(model, val_loader, device)\n",
        "        val_metrics = evaluate_binary(y_val_true, y_val_prob)\n",
        "        if val_metrics[\"f1\"] > best_val_f1:\n",
        "            best_val_f1 = val_metrics[\"f1\"]\n",
        "            best_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
        "        avg_loss = running_loss / max(1, steps_per_epoch)\n",
        "        print(f\"Epoch {epoch:02d}/{epochs} - loss: {avg_loss:.4f} - val_f1: {val_metrics['f1']:.4f} - val_acc: {val_metrics['accuracy']:.4f}\", flush=True)\n",
        "\n",
        "    if best_state is not None:\n",
        "        model.load_state_dict(best_state)\n",
        "\n",
        "    print(\"Final evaluation (val/test, t=0.50) ...\", flush=True)\n",
        "    y_val_true, y_val_prob = evaluate_model(model, val_loader, device)\n",
        "    y_test_true, y_test_prob = evaluate_model(model, test_loader, device)\n",
        "    val_metrics = evaluate_binary(y_val_true, y_val_prob)\n",
        "    test_metrics = evaluate_binary(y_test_true, y_test_prob)\n",
        "\n",
        "    print(\"\\nValidation metrics (t=0.50):\", flush=True)\n",
        "    for k, v in val_metrics.items():\n",
        "        print(f\"  {k:>9}: {v:.4f}\", flush=True)\n",
        "\n",
        "    best_t = find_best_threshold(y_val_true, y_val_prob, metric=\"f1\")\n",
        "    print(f\"\\nChosen decision threshold from validation (max F1): t = {best_t:.2f}\", flush=True)\n",
        "\n",
        "    def with_threshold(y_true: np.ndarray, y_prob: np.ndarray, t: float) -> dict:\n",
        "        y_pred = (y_prob >= t).astype(int)\n",
        "        return {\n",
        "            \"accuracy\": float(accuracy_score(y_true, y_pred)),\n",
        "            \"precision\": float(precision_score(y_true, y_pred, zero_division=0)),\n",
        "            \"recall\": float(recall_score(y_true, y_pred, zero_division=0)),\n",
        "            \"f1\": float(f1_score(y_true, y_pred, zero_division=0)),\n",
        "        }\n",
        "\n",
        "    val_tuned = with_threshold(y_val_true, y_val_prob, best_t)\n",
        "    test_tuned = with_threshold(y_test_true, y_test_prob, best_t)\n",
        "    print(\"\\nValidation (tuned threshold):\", flush=True)\n",
        "    for k, v in val_tuned.items():\n",
        "        print(f\"  {k:>9}: {v:.4f}\", flush=True)\n",
        "    print(\"\\nTest (tuned threshold):\", flush=True)\n",
        "    for k, v in test_tuned.items():\n",
        "        print(f\"  {k:>9}: {v:.4f}\", flush=True)\n",
        "\n",
        "\n",
        "def _parse_kernel_sizes(s: str) -> List[int]:\n",
        "    try:\n",
        "        return [int(x) for x in s.split(\",\") if x.strip()]\n",
        "    except Exception:\n",
        "        raise argparse.ArgumentTypeError(\"kernel_sizes must be a comma-separated list of integers, e.g. '3,4,5'\")\n",
        "\n",
        "\n",
        "def main() -> None:\n",
        "    parser = argparse.ArgumentParser(description=\"Baseline 4: Siamese TextCNN with clause/type split and optional dedup\")\n",
        "    parser.add_argument(\"--data_dir\", type=Path, default=Path(\"csv\"), help=\"Directory containing CSVs with clause_text, clause_type\")\n",
        "    parser.add_argument(\"--seed\", type=int, default=42, help=\"Random seed\")\n",
        "    parser.add_argument(\"--split_mode\", type=str, choices=[\"clause\", \"type\"], default=\"clause\", help=\"Split at clause level or hold out entire types\")\n",
        "    parser.add_argument(\"--dedup_texts\", action=\"store_true\", help=\"Deduplicate exact clause_text duplicates before splitting\")\n",
        "    parser.add_argument(\"--max_pairs_per_class\", type=int, default=200, help=\"Max positive pairs sampled per type (per split)\")\n",
        "    parser.add_argument(\"--max_len\", type=int, default=128, help=\"Maximum tokenized length per clause\")\n",
        "    parser.add_argument(\"--max_vocab_size\", type=int, default=30000, help=\"Maximum vocabulary size\")\n",
        "    parser.add_argument(\"--min_freq\", type=int, default=2, help=\"Minimum token frequency for vocab\")\n",
        "    parser.add_argument(\"--embedding_dim\", type=int, default=200, help=\"Embedding dimension\")\n",
        "    parser.add_argument(\"--cnn_channels\", type=int, default=128, help=\"Number of channels per convolutional kernel size\")\n",
        "    parser.add_argument(\"--kernel_sizes\", type=_parse_kernel_sizes, default=\"3,4,5\", help=\"Comma-separated kernel sizes, e.g., '3,4,5'\")\n",
        "    parser.add_argument(\"--mlp_hidden\", type=int, default=256, help=\"Hidden size of the MLP head\")\n",
        "    parser.add_argument(\"--batch_size\", type=int, default=64, help=\"Batch size\")\n",
        "    parser.add_argument(\"--epochs\", type=int, default=8, help=\"Training epochs\")\n",
        "    parser.add_argument(\"--lr\", type=float, default=1e-3, help=\"Learning rate\")\n",
        "    parser.add_argument(\"--log_interval\", type=int, default=200, help=\"Steps between training progress prints\")\n",
        "    args = parser.parse_args([])\n",
        "\n",
        "    run(\n",
        "        data_dir=args.data_dir,\n",
        "        seed=args.seed,\n",
        "        split_mode=args.split_mode,  # type: ignore[arg-type]\n",
        "        dedup_texts=args.dedup_texts,\n",
        "        max_pairs_per_class=args.max_pairs_per_class,\n",
        "        max_len=args.max_len,\n",
        "        max_vocab_size=args.max_vocab_size,\n",
        "        min_freq=args.min_freq,\n",
        "        embedding_dim=args.embedding_dim,\n",
        "        cnn_channels=args.cnn_channels,\n",
        "        kernel_sizes=args.kernel_sizes if isinstance(args.kernel_sizes, list) else _parse_kernel_sizes(args.kernel_sizes),  # type: ignore[arg-type]\n",
        "        mlp_hidden=args.mlp_hidden,\n",
        "        batch_size=args.batch_size,\n",
        "        epochs=args.epochs,\n",
        "        lr=args.lr,\n",
        "        log_interval=args.log_interval,\n",
        "    )\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5C7qcc53diQ1",
        "outputId": "67481471-6dc0-498a-9282-7d686a10c838"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading clauses from: csv\n",
            "Loaded 150881 clauses from 395 CSV file(s).\n",
            "Warning: Found 118 overlapping clause_texts between train and val. e.g.: ['time. time shall be of the essence of this agreement. except as otherwise set forth herein, specified times of day refer to new york city time.', 'other provisions. (a) no set-off', 'limitation of liability. neither pembroke nor its third party partners shall be liable for any direct, special, indirect, incidental, consequential, exemplary, extra-contractual, or punitive damages of any kind whatsoever, including, without limitation, lost revenues or lost profits, which may or do result from or relate to the use of, access to, or inability to use the report or any other products, services, materials, and information offered by pembroke under or in connection with this agreement, or the failure of pembroke to provide the report for any reason at any time, regardless of legal theory, whether or not any party had been advised of the possibility or probability of such damages, and even if the remedies otherwise available fail of their essential purpose. under no circumstances will the total liability of pembroke to purchaser or any other person or entity in connection with, based upon, or arising from the report or any other products, services, materials, and information offered by pembroke under or in connection with this agreement exceed, in the aggregate, the total of all amounts paid by purchaser to pembroke or its agents during the six-month period preceding the date the events giving rise to the liability arose.', 'interpretation. 1.1 in this agreement:', 'limitation of liability. except as expressly provided in an applicable service addendum, service provider and its affiliates shall not be liable for any indirect, incidental contingent, consequential, punitive, exemplary, special or similar damages, including but not limited to, loss of profits or loss of data, whether incurred as a result of negligence or otherwise, irrespective of whether service provider has been advised of the possibility of the incurrence by client of any such damages. service providers liability damages incurred in connection with services provided pursuant to this master agreement or any service addendum, including as a result of any negligence on the part of the service provider or its affiliates, shall not exceed three times the amount paid by client to service provider within the preceeding 12 month period for the particular service giving rise to such damages. further, service provider will have no liability for any cause of action against service provider which became known to client, or should have been known by client with reasonable investigation, within two years from the expiration or termination of this master agreement or applicable service addendum but client failed to provide actual notice to service provider within such two year period after the expiration or termination of this agreement or the applicable service addendum.']\n",
            "Warning: Found 126 overlapping clause_texts between train and test. e.g.: ['limitation of liability. (a) invesco powershares capital management shall not be liable for any error of judgment or mistake of law or for any loss suffered by any fund, the trust or any of its shareholders, in connection with the matters to which this agreement relates, except to the extent that such a loss results from willful misfeasance, bad faith or gross negligence on its part in the performance of its duties or from reckless disregard by it of its obligations and duties under this agreement. any person, even though also an officer, director, employee, or agent of invesco powershares capital management, who may be or become an officer, board member, employee or agent of the trust shall be deemed, when rendering services to any fund or the trust or acting with respect to any business of such fund or the trust, to be rendering such service to or acting solely for the fund or the trust and not as an officer, director, employee, or agent or one under the control or direction of invesco powershares capital management even though paid by it.', 'limitation of liability. you agree that we will not be liable for any direct, indirect, incidental, special, consequential or exemplary damages, including, but not limited to, damages for loss of profits, goodwill, use, data or other losses incurred by you or any third party arising from or related to the use of, inability to use, or the termination of the use of any remote banking service, regardless of the form of action or claim (whether contract, tort, strict liability or otherwise), even if we have been informed of the possibility thereof, except as otherwise required by law.', 'limitation of liability. under no circumstances and under no legal theory, whether tort (including, without limitation, negligence or strict liability), contract, or otherwise, shall sgi or any sgi licensor be liable for any claim, damages or other liability, whether in an action of contract, tort or otherwise, arising from, out of or in connection with the subject software or the use or other dealings in the subjec software. some jurisdictions do not allow the exclusion or limitation of certain damages, so this exclusion and limitation may not apply to recipient to the', 'limitation of liability. to the fullest extent available under applicable law, mlss entire and cumulative liability to participant or avp, or any third party, for any loss or damages resulting from any claims, demands, or actions arising out of or relating to this agreement, or the use or display of the mls listing information, including any tort, such as negligence, shall not exceed an amount equal to the license fee paid to mls under this agreement during the one (1) year period immediately preceding the date on which the claim accrued. without waiver of the limitations set forth in this agreement, in no event shall mls be liable for any indirect, incidental, consequential, special, or exemplary damages or lost profits, even if mls has been advised of the possibility of such damages.', 'limitation of liability. the limitations and exclusions in this section 10 (limitation of liability) apply to the full extent they are not prohibited by applicable law without the possibility of contractual waiver.']\n",
            "Warning: Found 26 overlapping clause_texts between val and test. e.g.: ['limitation of liability. in no event will either party be liable for any indirect, incidental, special, consequential or punitive damages, or damages for loss of profits, revenue, business, savings, data, use or cost of substitute procurement, incurred by either party or any third party, whether in an action in contract or tort, even if the other party has been advised of the possibility of such damages or if such damages are foreseeable. in no event will companys liability for damages hereunder exceed the lesser of (i) amounts actually paid by you to company for the service or us$500. the parties acknowledge that the limitations of liability in this section and in the other provisions of this agreement and the allocation of risk herein are an essential element of this transaction between the parties, without which company would not have entered into this agreement. companys pricing reflects this allocation of risk and the limitation of liability specified herein. some states/countries do not allow the exclusion or limitation of incidental or consequential damages, so the above limitations and/or exclusion or limitation of liability may not apply to you.', 'limitation of liability. except to the extent specifically provided in any ancillary agreement, in no event shall either party be liable to the other party for any exemplary, punitive, special, indirect, consequential, remote or speculative damages (including in respect of lost profits or revenues), however caused and on any theory of liability (including negligence) arising in any way out of any provision of this agreement, whether or not such party has been advised of the possibility of such damages.', 'limitation of liability. the members of the board shall not be responsible or liable in any event for any neglect or wrongdoing of any officer, agent, employee, adviser or principal underwriter of the fund, nor shall any member be responsible for the act or omission of any other member, but nothing herein contained shall protect any member against any liability to which he or she would otherwise be subject by reason of willful misfeasance, bad faith, gross negligence or reckless disregard of the duties involved in the conduct of his or her office. every note, bond, contract, instrument, certificate, share or undertaking and every other act or thing whatsoever executed or done by on behalf of the fund or the board or any of them in connection with the fund shall be conclusively deemed to have been executed or done only in or with respect to their or his or her capacity as members or a member, and such members or member shall not be personally liable thereon.', 'costs. each party must pay its own costs of negotiating, preparing and executing this agreement.', 'limitation of liability. the term \"']\n",
            "Split clauses -> train/val/test: 105616/22632/22633. Built pairs -> train/val/test: 105306/22438/22438.\n",
            "Vocab size: 30000 (min_freq=2, max_vocab_size=30000)\n",
            "Using device: cuda\n",
            "Starting training for 8 epoch(s), steps per epoch: 1646\n",
            "Epoch 01/8 - step 00200/01646 loss: 0.6842 (avg: 0.6985) - elapsed: 92.8s\n",
            "Epoch 01/8 - step 00400/01646 loss: 0.6733 (avg: 0.6891) - elapsed: 187.3s\n",
            "Epoch 01/8 - step 00600/01646 loss: 0.6498 (avg: 0.6742) - elapsed: 282.1s\n",
            "Epoch 01/8 - step 00800/01646 loss: 0.5053 (avg: 0.6524) - elapsed: 377.0s\n",
            "Epoch 01/8 - step 01000/01646 loss: 0.5289 (avg: 0.6328) - elapsed: 471.8s\n",
            "Epoch 01/8 - step 01200/01646 loss: 0.5145 (avg: 0.6136) - elapsed: 566.6s\n",
            "Epoch 01/8 - step 01400/01646 loss: 0.4789 (avg: 0.5959) - elapsed: 661.4s\n",
            "Epoch 01/8 - step 01600/01646 loss: 0.4687 (avg: 0.5811) - elapsed: 756.1s\n",
            "Epoch 01/8 - step 01646/01646 loss: 0.4927 (avg: 0.5774) - elapsed: 777.8s\n",
            "Evaluating on validation...\n",
            "Epoch 01/8 - loss: 0.5774 - val_f1: 0.8033 - val_acc: 0.7811\n",
            "Epoch 02/8 - step 00200/01646 loss: 0.3766 (avg: 0.4201) - elapsed: 94.6s\n",
            "Epoch 02/8 - step 00400/01646 loss: 0.3111 (avg: 0.4105) - elapsed: 189.2s\n",
            "Epoch 02/8 - step 00600/01646 loss: 0.2977 (avg: 0.4060) - elapsed: 283.8s\n",
            "Epoch 02/8 - step 00800/01646 loss: 0.4545 (avg: 0.3989) - elapsed: 378.3s\n",
            "Epoch 02/8 - step 01000/01646 loss: 0.3340 (avg: 0.3896) - elapsed: 472.8s\n",
            "Epoch 02/8 - step 01200/01646 loss: 0.4128 (avg: 0.3846) - elapsed: 567.3s\n",
            "Epoch 02/8 - step 01400/01646 loss: 0.3501 (avg: 0.3783) - elapsed: 661.8s\n",
            "Epoch 02/8 - step 01600/01646 loss: 0.3589 (avg: 0.3714) - elapsed: 756.3s\n",
            "Epoch 02/8 - step 01646/01646 loss: 0.4687 (avg: 0.3696) - elapsed: 777.9s\n",
            "Evaluating on validation...\n",
            "Epoch 02/8 - loss: 0.3696 - val_f1: 0.8757 - val_acc: 0.8666\n",
            "Epoch 03/8 - step 00200/01646 loss: 0.2550 (avg: 0.2562) - elapsed: 94.4s\n",
            "Epoch 03/8 - step 00400/01646 loss: 0.2418 (avg: 0.2557) - elapsed: 188.9s\n",
            "Epoch 03/8 - step 00600/01646 loss: 0.2833 (avg: 0.2572) - elapsed: 283.3s\n",
            "Epoch 03/8 - step 00800/01646 loss: 0.2883 (avg: 0.2558) - elapsed: 377.7s\n",
            "Epoch 03/8 - step 01000/01646 loss: 0.2111 (avg: 0.2519) - elapsed: 472.0s\n",
            "Epoch 03/8 - step 01200/01646 loss: 0.2955 (avg: 0.2508) - elapsed: 566.4s\n",
            "Epoch 03/8 - step 01400/01646 loss: 0.1969 (avg: 0.2484) - elapsed: 660.7s\n",
            "Epoch 03/8 - step 01600/01646 loss: 0.1693 (avg: 0.2472) - elapsed: 755.0s\n",
            "Epoch 03/8 - step 01646/01646 loss: 0.1030 (avg: 0.2463) - elapsed: 776.5s\n",
            "Evaluating on validation...\n",
            "Epoch 03/8 - loss: 0.2463 - val_f1: 0.9230 - val_acc: 0.9204\n",
            "Epoch 04/8 - step 00200/01646 loss: 0.1576 (avg: 0.1814) - elapsed: 94.3s\n",
            "Epoch 04/8 - step 00400/01646 loss: 0.1288 (avg: 0.1756) - elapsed: 188.5s\n",
            "Epoch 04/8 - step 00600/01646 loss: 0.2104 (avg: 0.1751) - elapsed: 282.7s\n",
            "Epoch 04/8 - step 00800/01646 loss: 0.2475 (avg: 0.1756) - elapsed: 376.9s\n",
            "Epoch 04/8 - step 01000/01646 loss: 0.1539 (avg: 0.1744) - elapsed: 471.0s\n",
            "Epoch 04/8 - step 01200/01646 loss: 0.1269 (avg: 0.1739) - elapsed: 565.2s\n",
            "Epoch 04/8 - step 01400/01646 loss: 0.1786 (avg: 0.1741) - elapsed: 659.4s\n",
            "Epoch 04/8 - step 01600/01646 loss: 0.1403 (avg: 0.1732) - elapsed: 753.5s\n",
            "Epoch 04/8 - step 01646/01646 loss: 0.1042 (avg: 0.1729) - elapsed: 775.0s\n",
            "Evaluating on validation...\n",
            "Epoch 04/8 - loss: 0.1729 - val_f1: 0.9293 - val_acc: 0.9258\n",
            "Epoch 05/8 - step 00200/01646 loss: 0.1616 (avg: 0.1188) - elapsed: 94.0s\n",
            "Epoch 05/8 - step 00400/01646 loss: 0.1263 (avg: 0.1235) - elapsed: 188.1s\n",
            "Epoch 05/8 - step 00600/01646 loss: 0.1969 (avg: 0.1242) - elapsed: 282.1s\n",
            "Epoch 05/8 - step 00800/01646 loss: 0.1024 (avg: 0.1261) - elapsed: 376.2s\n",
            "Epoch 05/8 - step 01000/01646 loss: 0.1254 (avg: 0.1275) - elapsed: 470.3s\n",
            "Epoch 05/8 - step 01200/01646 loss: 0.2004 (avg: 0.1279) - elapsed: 564.3s\n",
            "Epoch 05/8 - step 01400/01646 loss: 0.0466 (avg: 0.1283) - elapsed: 658.3s\n",
            "Epoch 05/8 - step 01600/01646 loss: 0.4130 (avg: 0.1286) - elapsed: 752.4s\n",
            "Epoch 05/8 - step 01646/01646 loss: 0.0845 (avg: 0.1290) - elapsed: 773.9s\n",
            "Evaluating on validation...\n",
            "Epoch 05/8 - loss: 0.1290 - val_f1: 0.9485 - val_acc: 0.9469\n",
            "Epoch 06/8 - step 00200/01646 loss: 0.0301 (avg: 0.0937) - elapsed: 94.0s\n",
            "Epoch 06/8 - step 00400/01646 loss: 0.0955 (avg: 0.0960) - elapsed: 188.1s\n",
            "Epoch 06/8 - step 00600/01646 loss: 0.0936 (avg: 0.1001) - elapsed: 282.0s\n",
            "Epoch 06/8 - step 00800/01646 loss: 0.1506 (avg: 0.1019) - elapsed: 376.4s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Baseline 3 :  Siamese Self Attention Encoder"
      ],
      "metadata": {
        "id": "mpMURJ383ymx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "dWvXo4zb0dkA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "project_path = '/content/drive/MyDrive/A2'\n",
        "sys.path.append(project_path)\n"
      ],
      "metadata": {
        "id": "6pc44brh0vRD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy pandas torch scikit-learn tqdm matplotlib"
      ],
      "metadata": {
        "id": "PrhggUBx0w_g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/drive/MyDrive/A2/a2.py --data_dir /content/drive/MyDrive/A2/datafiles --epochs 50 --max_len 128 --max_vocab_size 30000 --embedding_dim 200 --num_heads 4 --mlp_hidden 256 --batch_size 64\n",
        "\n"
      ],
      "metadata": {
        "id": "feMD5tlO00c0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import random\n",
        "import re\n",
        "from dataclasses import dataclass\n",
        "from pathlib import Path\n",
        "from typing import List, Tuple\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        "    f1_score,\n",
        "    roc_auc_score,\n",
        "    average_precision_score,\n",
        "    roc_curve,\n",
        "    auc\n",
        ")\n",
        "\n",
        "\n",
        "# =========================\n",
        "# Utility Classes & Helpers\n",
        "# =========================\n",
        "\n",
        "@dataclass\n",
        "class Clause:\n",
        "    text: str\n",
        "    clause_type: str\n",
        "    source_file: str\n",
        "\n",
        "\n",
        "def set_global_seeds(seed: int) -> None:\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "\n",
        "def load_clauses_from_folder(folder: Path) -> List[Clause]:\n",
        "    csv_files = sorted(folder.rglob(\"*.csv\"))\n",
        "    clauses: List[Clause] = []\n",
        "    for csv_path in csv_files:\n",
        "        try:\n",
        "            df = pd.read_csv(csv_path)\n",
        "        except Exception as exc:\n",
        "            print(f\"Warning: failed to read {csv_path.name}: {exc}\")\n",
        "            continue\n",
        "\n",
        "        if \"clause_text\" not in df.columns or \"clause_type\" not in df.columns:\n",
        "            print(f\"Warning: {csv_path.name} missing required columns.\")\n",
        "            continue\n",
        "\n",
        "        for _, row in df.iterrows():\n",
        "            text = str(row[\"clause_text\"]) if not pd.isna(row[\"clause_text\"]) else \"\"\n",
        "            ctype = str(row[\"clause_type\"]) if not pd.isna(row[\"clause_type\"]) else \"\"\n",
        "            if text.strip():\n",
        "                clauses.append(Clause(text.strip(), ctype.strip(), csv_path.name))\n",
        "    return clauses\n",
        "\n",
        "\n",
        "def make_balanced_pairs(clauses: List[Clause], max_pairs_per_class: int | None, rng: np.random.Generator):\n",
        "    type_to_indices: dict[str, List[int]] = {}\n",
        "    for idx, c in enumerate(clauses):\n",
        "        type_to_indices.setdefault(c.clause_type, []).append(idx)\n",
        "\n",
        "    pos_pairs = []\n",
        "    for ctype, idxs in type_to_indices.items():\n",
        "        if len(idxs) < 2:\n",
        "            continue\n",
        "        rng.shuffle(idxs)\n",
        "        pair_count = len(idxs) // 2\n",
        "        if max_pairs_per_class is not None:\n",
        "            pair_count = min(pair_count, max_pairs_per_class)\n",
        "        for i in range(pair_count):\n",
        "            a, b = idxs[2 * i], idxs[2 * i + 1]\n",
        "            pos_pairs.append((a, b))\n",
        "\n",
        "    neg_pairs = []\n",
        "    all_idx = np.arange(len(clauses))\n",
        "    while len(neg_pairs) < len(pos_pairs):\n",
        "        i, j = rng.choice(all_idx, 2, replace=False)\n",
        "        if clauses[i].clause_type != clauses[j].clause_type:\n",
        "            neg_pairs.append((i, j))\n",
        "\n",
        "    X, y = [], []\n",
        "    for i, j in pos_pairs:\n",
        "        X.append((clauses[i].text, clauses[j].text))\n",
        "        y.append(1)\n",
        "    for i, j in neg_pairs:\n",
        "        X.append((clauses[i].text, clauses[j].text))\n",
        "        y.append(0)\n",
        "    order = rng.permutation(len(X))\n",
        "    X = [X[k] for k in order]\n",
        "    y = np.array([y[k] for k in order])\n",
        "    return X, y\n",
        "\n",
        "\n",
        "def build_vocab(texts: List[str], max_vocab_size: int, min_freq: int):\n",
        "    token_freq = {}\n",
        "    for t in texts:\n",
        "        for tok in re.findall(r\"\\b\\w+\\b\", t.lower()):\n",
        "            token_freq[tok] = token_freq.get(tok, 0) + 1\n",
        "\n",
        "    sorted_tokens = sorted([tok for tok, f in token_freq.items() if f >= min_freq],\n",
        "                           key=lambda x: (-token_freq[x], x))\n",
        "    limited = sorted_tokens[: max_vocab_size - 2]\n",
        "    token_to_id = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
        "    for i, tok in enumerate(limited, start=2):\n",
        "        token_to_id[tok] = i\n",
        "    return token_to_id\n",
        "\n",
        "\n",
        "def texts_to_ids(texts: List[str], token_to_id: dict, max_len: int):\n",
        "    pad, unk = 0, 1\n",
        "    seqs = np.full((len(texts), max_len), pad, dtype=np.int64)\n",
        "    lens = np.zeros(len(texts), dtype=np.int64)\n",
        "    for i, t in enumerate(texts):\n",
        "        tokens = re.findall(r\"\\b\\w+\\b\", t.lower())\n",
        "        ids = [token_to_id.get(tok, unk) for tok in tokens][:max_len]\n",
        "        seqs[i, :len(ids)] = ids\n",
        "        lens[i] = len(ids)\n",
        "    return seqs, lens\n",
        "\n",
        "\n",
        "# ======================\n",
        "# Dataset\n",
        "# ======================\n",
        "\n",
        "class PairDataset(Dataset):\n",
        "    def __init__(self, pairs, token_to_id, max_len, labels=None):\n",
        "        self.left_texts = [a for a, _ in pairs]\n",
        "        self.right_texts = [b for _, b in pairs]\n",
        "        self.labels = labels.astype(np.int64) if labels is not None else None\n",
        "        self.token_to_id = token_to_id\n",
        "        self.max_len = max_len\n",
        "        self.left_ids, self.left_lens = texts_to_ids(self.left_texts, token_to_id, max_len)\n",
        "        self.right_ids, self.right_lens = texts_to_ids(self.right_texts, token_to_id, max_len)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.left_texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        l_ids = torch.tensor(self.left_ids[idx])\n",
        "        r_ids = torch.tensor(self.right_ids[idx])\n",
        "        l_len = int(self.left_lens[idx])\n",
        "        r_len = int(self.right_lens[idx])\n",
        "        if self.labels is None:\n",
        "            return l_ids, l_len, r_ids, r_len\n",
        "        return l_ids, l_len, r_ids, r_len, int(self.labels[idx])\n",
        "\n",
        "\n",
        "# ======================\n",
        "# Self-Attention Encoder\n",
        "# ======================\n",
        "\n",
        "class SelfAttentionEncoder(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, num_heads=4, dropout=0.1, pad_idx=0):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_idx)\n",
        "        self.attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout, batch_first=True)\n",
        "        self.norm = nn.LayerNorm(embed_dim)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(embed_dim, embed_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, ids, lengths):\n",
        "        mask = torch.arange(ids.size(1), device=ids.device)[None, :] >= lengths[:, None]\n",
        "        x = self.embedding(ids)\n",
        "        attn_out, _ = self.attn(x, x, x, key_padding_mask=mask)\n",
        "        x = self.norm(x + attn_out)\n",
        "        x = self.fc(x)\n",
        "        mask_inv = (~mask).unsqueeze(-1)\n",
        "        pooled = (x * mask_inv).sum(1) / mask_inv.sum(1).clamp_min(1e-6)\n",
        "        return pooled\n",
        "\n",
        "\n",
        "# ======================\n",
        "# Siamese Model\n",
        "# ======================\n",
        "\n",
        "class SiameseSelfAttention(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, num_heads, mlp_hidden, dropout=0.2):\n",
        "        super().__init__()\n",
        "        self.encoder = SelfAttentionEncoder(vocab_size, embed_dim, num_heads, dropout)\n",
        "        enc_dim = embed_dim\n",
        "        feat_dim = enc_dim * 4 + 1\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(feat_dim, mlp_hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(mlp_hidden, mlp_hidden // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(mlp_hidden // 2, 1),\n",
        "        )\n",
        "\n",
        "    @staticmethod\n",
        "    def cosine(u, v, eps=1e-12):\n",
        "        num = (u * v).sum(dim=1, keepdim=True)\n",
        "        den = (u.norm(dim=1, keepdim=True) * v.norm(dim=1, keepdim=True)).clamp_min(eps)\n",
        "        return num / den\n",
        "\n",
        "    def forward(self, l_ids, l_len, r_ids, r_len):\n",
        "        u = self.encoder(l_ids, l_len)\n",
        "        v = self.encoder(r_ids, r_len)\n",
        "        abs_diff = torch.abs(u - v)\n",
        "        hadamard = u * v\n",
        "        cos = self.cosine(u, v)\n",
        "        feats = torch.cat([u, v, abs_diff, hadamard, cos], dim=1)\n",
        "        return self.mlp(feats).squeeze(1)\n",
        "\n",
        "\n",
        "# ======================\n",
        "# Evaluation helpers\n",
        "# ======================\n",
        "\n",
        "def evaluate_binary(y_true, y_proba):\n",
        "    y_pred = (y_proba >= 0.5).astype(int)\n",
        "    return {\n",
        "        \"accuracy\": float(accuracy_score(y_true, y_pred)),\n",
        "        \"precision\": float(precision_score(y_true, y_pred, zero_division=0)),\n",
        "        \"recall\": float(recall_score(y_true, y_pred, zero_division=0)),\n",
        "        \"f1\": float(f1_score(y_true, y_pred, zero_division=0)),\n",
        "        \"roc_auc\": float(roc_auc_score(y_true, y_proba)),\n",
        "        \"pr_auc\": float(average_precision_score(y_true, y_proba)),\n",
        "    }\n",
        "\n",
        "\n",
        "def evaluate_model(model, loader, device):\n",
        "    model.eval()\n",
        "    all_probs, all_labels = [], []\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(loader, desc=\"Evaluating\", leave=False):\n",
        "            l_ids, l_len, r_ids, r_len, y = batch\n",
        "            l_ids, r_ids = l_ids.to(device), r_ids.to(device)\n",
        "            l_len, r_len, y = l_len.to(device), r_len.to(device), y.to(device)\n",
        "            probs = torch.sigmoid(model(l_ids, l_len, r_ids, r_len))\n",
        "            all_probs.extend(probs.cpu().numpy())\n",
        "            all_labels.extend(y.cpu().numpy())\n",
        "    return np.array(all_labels), np.array(all_probs)\n",
        "\n",
        "\n",
        "# ======================\n",
        "# Training Function\n",
        "# ======================\n",
        "def run(data_dir, seed, max_pairs_per_class, max_len, max_vocab_size, min_freq,\n",
        "        embedding_dim, num_heads, mlp_hidden, batch_size, epochs, lr):\n",
        "\n",
        "    set_global_seeds(seed)\n",
        "    print(\"Loading dataset...\")\n",
        "    clauses = load_clauses_from_folder(data_dir)\n",
        "    print(f\"Loaded {len(clauses)} clauses from {len(set(c.source_file for c in clauses))} files.\")\n",
        "\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    idx = np.arange(len(clauses))\n",
        "    try:\n",
        "        idx_train, idx_temp = train_test_split(idx, test_size=0.3, stratify=[c.clause_type for c in clauses], random_state=seed)\n",
        "        idx_val, idx_test = train_test_split(idx_temp, test_size=0.5, stratify=[c.clause_type for c in [clauses[i] for i in idx_temp]], random_state=seed)\n",
        "    except Exception:\n",
        "        idx_train, idx_temp = train_test_split(idx, test_size=0.3, random_state=seed)\n",
        "        idx_val, idx_test = train_test_split(idx_temp, test_size=0.5, random_state=seed)\n",
        "\n",
        "    clauses_train = [clauses[i] for i in idx_train]\n",
        "    clauses_val = [clauses[i] for i in idx_val]\n",
        "    clauses_test = [clauses[i] for i in idx_test]\n",
        "\n",
        "    rng = np.random.default_rng(seed)\n",
        "    X_train, y_train = make_balanced_pairs(clauses_train, max_pairs_per_class, rng)\n",
        "    X_val, y_val = make_balanced_pairs(clauses_val, max_pairs_per_class, rng)\n",
        "    X_test, y_test = make_balanced_pairs(clauses_test, max_pairs_per_class, rng)\n",
        "\n",
        "    token_to_id = build_vocab([a for a, _ in X_train] + [b for _, b in X_train], max_vocab_size, min_freq)\n",
        "    vocab_size = len(token_to_id)\n",
        "    print(f\"Vocab size: {vocab_size}\")\n",
        "\n",
        "    ds_train = PairDataset(X_train, token_to_id, max_len, y_train)\n",
        "    ds_val = PairDataset(X_val, token_to_id, max_len, y_val)\n",
        "    ds_test = PairDataset(X_test, token_to_id, max_len, y_test)\n",
        "\n",
        "    def collate(batch):\n",
        "        l_ids, l_len, r_ids, r_len, y = zip(*batch)\n",
        "        return (\n",
        "            torch.stack(l_ids),\n",
        "            torch.tensor(l_len),\n",
        "            torch.stack(r_ids),\n",
        "            torch.tensor(r_len),\n",
        "            torch.tensor(y),\n",
        "        )\n",
        "\n",
        "    train_loader = DataLoader(ds_train, batch_size=batch_size, shuffle=True, collate_fn=collate)\n",
        "    val_loader = DataLoader(ds_val, batch_size=batch_size, shuffle=False, collate_fn=collate)\n",
        "    test_loader = DataLoader(ds_test, batch_size=batch_size, shuffle=False, collate_fn=collate)\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(\"Using device:\", device)\n",
        "\n",
        "    model = SiameseSelfAttention(vocab_size, embedding_dim, num_heads, mlp_hidden, dropout=0.2).to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    loss_fn = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    # Track training progress\n",
        "    train_losses = []\n",
        "    val_f1_scores = []\n",
        "    best_f1 = -1\n",
        "    best_state = None\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch}/{epochs}\", leave=False):\n",
        "            l_ids, l_len, r_ids, r_len, y = batch\n",
        "            l_ids, r_ids = l_ids.to(device), r_ids.to(device)\n",
        "            l_len, r_len, y = l_len.to(device), r_len.to(device), y.float().to(device)\n",
        "\n",
        "            logits = model(l_ids, l_len, r_ids, r_len)\n",
        "            loss = loss_fn(logits, y)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        avg_train_loss = total_loss / len(train_loader)\n",
        "        train_losses.append(avg_train_loss)\n",
        "\n",
        "        y_val_true, y_val_prob = evaluate_model(model, val_loader, device)\n",
        "        val_metrics = evaluate_binary(y_val_true, y_val_prob)\n",
        "        val_f1_scores.append(val_metrics[\"f1\"])\n",
        "\n",
        "        print(f\"Epoch {epoch:02d} | loss={avg_train_loss:.4f} | val_f1={val_metrics['f1']:.4f}\")\n",
        "\n",
        "        if val_metrics[\"f1\"] > best_f1:\n",
        "            best_f1 = val_metrics[\"f1\"]\n",
        "            best_state = {k: v.cpu() for k, v in model.state_dict().items()}\n",
        "\n",
        "    # Restore best weights\n",
        "    if best_state:\n",
        "        model.load_state_dict(best_state)\n",
        "\n",
        "    # Plot training graphs\n",
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    plt.plot(range(1, len(train_losses)+1), train_losses, marker='o', label='Training Loss')\n",
        "    plt.title(\"Training Loss per Epoch\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "    plt.savefig(\"/content/drive/MyDrive/A2/training_loss.png\")\n",
        "    plt.close()\n",
        "\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    plt.plot(range(1, len(val_f1_scores)+1), val_f1_scores, marker='o', color='orange', label='Validation F1-Score')\n",
        "    plt.title(\"Validation F1-Score per Epoch\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"F1-Score\")\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "    plt.savefig(\"/content/drive/MyDrive/A2/val_f1.png\")\n",
        "    plt.close()\n",
        "\n",
        "    # Final test evaluation\n",
        "    y_test_true, y_test_prob = evaluate_model(model, test_loader, device)\n",
        "    test_metrics = evaluate_binary(y_test_true, y_test_prob)\n",
        "    print(\"\\nFinal Test Metrics:\")\n",
        "    for k, v in test_metrics.items():\n",
        "        print(f\"{k:>10}: {v:.4f}\")\n",
        "\n",
        "    # ROC Curve\n",
        "    from sklearn.metrics import roc_curve, auc\n",
        "    fpr, tpr, _ = roc_curve(y_test_true, y_test_prob)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "\n",
        "    plt.figure(figsize=(6, 5))\n",
        "    plt.plot(fpr, tpr, label=f\"ROC Curve (AUC = {roc_auc:.3f})\")\n",
        "    plt.plot([0, 1], [0, 1], linestyle='--', color='gray')\n",
        "    plt.xlabel(\"False Positive Rate\")\n",
        "    plt.ylabel(\"True Positive Rate\")\n",
        "    plt.title(\"ROC Curve on Test Set\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.savefig(\"/content/drive/MyDrive/A2/roc_curve.png\")\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser(description=\"Siamese Self-Attention Encoder Baseline\")\n",
        "    parser.add_argument(\"--data_dir\", type=Path, default=Path(\"datafiles\"))\n",
        "    parser.add_argument(\"--seed\", type=int, default=42)\n",
        "    parser.add_argument(\"--max_pairs_per_class\", type=int, default=200)\n",
        "    parser.add_argument(\"--max_len\", type=int, default=128)\n",
        "    parser.add_argument(\"--max_vocab_size\", type=int, default=30000)\n",
        "    parser.add_argument(\"--min_freq\", type=int, default=2)\n",
        "    parser.add_argument(\"--embedding_dim\", type=int, default=200)\n",
        "    parser.add_argument(\"--num_heads\", type=int, default=4)\n",
        "    parser.add_argument(\"--mlp_hidden\", type=int, default=256)\n",
        "    parser.add_argument(\"--batch_size\", type=int, default=64)\n",
        "    parser.add_argument(\"--epochs\", type=int, default=8)\n",
        "    parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    run(**vars(args))\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "zWdGaVmo03Xw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading dataset...\n",
        "Loaded 150881 clauses from 395 files.\n",
        "\n",
        "Vocab size: 30000\n",
        "\n",
        "Using device: cuda\n",
        "\n",
        "Epoch 01 | loss=0.3763 | val_f1=0.8993\n",
        "\n",
        "Epoch 02 | loss=0.1894 | val_f1=0.9330\n",
        "\n",
        "Epoch 03 | loss=0.1332 | val_f1=0.9352\n",
        "\n",
        "Epoch 04 | loss=0.1034 | val_f1=0.9410\n",
        "\n",
        "Epoch 05 | loss=0.0829 | val_f1=0.9435\n",
        "\n",
        "Epoch 06 | loss=0.0674 | val_f1=0.9383\n",
        "\n",
        "Epoch 07 | loss=0.0592 | val_f1=0.9440\n",
        "\n",
        "Epoch 08 | loss=0.0509 | val_f1=0.9448\n",
        "\n",
        "Epoch 09 | loss=0.0469 | val_f1=0.9415\n",
        "\n",
        "Epoch 10 | loss=0.0393 | val_f1=0.9452\n",
        "\n",
        "Epoch 11 | loss=0.0385 | val_f1=0.9428\n",
        "\n",
        "Epoch 12 | loss=0.0351 | val_f1=0.9419\n",
        "\n",
        "Epoch 13 | loss=0.0342 | val_f1=0.9416\n",
        "\n",
        "Epoch 14 | loss=0.0307 | val_f1=0.9436\n",
        "\n",
        "Epoch 15 | loss=0.0280 | val_f1=0.9419\n",
        "\n",
        "Epoch 16 | loss=0.0270 | val_f1=0.9444\n",
        "\n",
        "Epoch 17 | loss=0.0259 | val_f1=0.9432\n",
        "\n",
        "Epoch 18 | loss=0.0246 | val_f1=0.9427\n",
        "\n",
        "Epoch 19 | loss=0.0243 | val_f1=0.9421\n",
        "\n",
        "Epoch 20 | loss=0.0231 | val_f1=0.9434\n",
        "\n",
        "Epoch 21 | loss=0.0219 | val_f1=0.9406\n",
        "\n",
        "Epoch 22 | loss=0.0216 | val_f1=0.9362\n",
        "\n",
        "Epoch 23 | loss=0.0212 | val_f1=0.9435\n",
        "\n",
        "Epoch 24 | loss=0.0204 | val_f1=0.9440\n",
        "\n",
        "Epoch 25 | loss=0.0177 | val_f1=0.9415\n",
        "\n",
        "Epoch 26 | loss=0.0200 | val_f1=0.9394\n",
        "\n",
        "Epoch 27 | loss=0.0180 | val_f1=0.9434\n",
        "\n",
        "Epoch 28 | loss=0.0190 | val_f1=0.9405\n",
        "\n",
        "Epoch 29 | loss=0.0179 | val_f1=0.9404\n",
        "\n",
        "Epoch 30 | loss=0.0175 | val_f1=0.9413\n",
        "\n",
        "Epoch 31 | loss=0.0173 | val_f1=0.9427\n",
        "\n",
        "Epoch 32 | loss=0.0158 | val_f1=0.9407\n",
        "\n",
        "Epoch 33 | loss=0.0146 | val_f1=0.9422\n",
        "\n",
        "Epoch 34 | loss=0.0155 | val_f1=0.9417\n",
        "\n",
        "Epoch 35 | loss=0.0158 | val_f1=0.9399\n",
        "\n",
        "Epoch 36 | loss=0.0157 | val_f1=0.9385\n",
        "\n",
        "Epoch 37 | loss=0.0135 | val_f1=0.9412\n",
        "\n",
        "Epoch 38 | loss=0.0138 | val_f1=0.9423\n",
        "\n",
        "Epoch 39 | loss=0.0149 | val_f1=0.9408\n",
        "\n",
        "Epoch 40 | loss=0.0140 | val_f1=0.9402\n",
        "\n",
        "Epoch 41 | loss=0.0124 | val_f1=0.9421\n",
        "\n",
        "Epoch 42 | loss=0.0118 | val_f1=0.9418\n",
        "\n",
        "Epoch 43 | loss=0.0123 | val_f1=0.9410\n",
        "\n",
        "Epoch 44 | loss=0.0140 | val_f1=0.9423\n",
        "\n",
        "Epoch 45 | loss=0.0121 | val_f1=0.9375\n",
        "\n",
        "Epoch 46 | loss=0.0116 | val_f1=0.9404\n",
        "\n",
        "Epoch 47 | loss=0.0123 | val_f1=0.9390\n",
        "\n",
        "Epoch 48 | loss=0.0120 | val_f1=0.9400\n",
        "\n",
        "Epoch 49 | loss=0.0111 | val_f1=0.9421\n",
        "\n",
        "Epoch 50 | loss=0.0118 | val_f1=0.9425\n",
        "                                                  \n",
        "Final Test Metrics:\n",
        "  \n",
        " accuracy: 0.9438\n",
        "\n",
        " precision: 0.9479\n",
        "\n",
        " recall: 0.9391\n",
        "\n",
        " f1: 0.9435\n",
        "\n",
        " roc_auc: 0.9850\n",
        "\n",
        " pr_auc: 0.9838"
      ],
      "metadata": {
        "id": "06VUmovT1Dd3"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HJIJ1aF_1HFz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}